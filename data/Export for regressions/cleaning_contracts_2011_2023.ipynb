{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning contract data (2011-2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "- No conctracts that are entirely after 2024 or before 2010.\n",
    "- Only main tracks (nhsp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we clean up the data from the raw data compilation of maintenance contracts (produced by running the R code on the excel file with BAS contracts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the raw data from the folder in raw_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "\n",
    "# Step 1: Load the Excel file containing service contracts for each bandel\n",
    "excel_file_path = \"../Python matching/raw_data/more_servicekontrakt_per_bandel_regression.xlsx\"\n",
    "\n",
    "#sheet_name = \"uppdaterad\"\n",
    "sheet_name = \"tid per bandel\"\n",
    "\n",
    "# Read the specific sheet 'T24' into a DataFrame\n",
    "servicekontrakt_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no information, these are normally simply indicating the name of the contract region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where the third column is missing \n",
    "servicekontrakt_df = servicekontrakt_df[servicekontrakt_df.iloc[:, 2].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the column Tidsperiod (e.g., 2024-2030) to extract start_year (2024) and end_year (2030)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the column Tidsperiod (20XX - 20YY) into two new columns 'Start_year' and 'End_year'\n",
    "def parse_tidsperiod(tidsperiod):\n",
    "    import re\n",
    "    # first remove spaces in tidsperiod\n",
    "    tidsperiod = tidsperiod.replace(' ', '')\n",
    "    tidsperiod_match = re.match(r'^(\\d{4})-(\\d{4})$', tidsperiod)\n",
    "    if tidsperiod_match:\n",
    "        start_year = int(tidsperiod_match.group(1))\n",
    "        end_year = int(tidsperiod_match.group(2))\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "    return pd.Series([start_year, end_year])\n",
    "\n",
    "# Apply the parsing function to create two new columns\n",
    "servicekontrakt_df[['Start_year', 'End_year']] = servicekontrakt_df['Tidsperiod'].apply(parse_tidsperiod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we focus on contracts that finish at latest 2023, we remove all other contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where start_year is 2024 or later\n",
    "servicekontrakt_df_cleaned = servicekontrakt_df[servicekontrakt_df['Start_year'] < 2024]\n",
    "# remove all contracts where end_year is before 2010\n",
    "servicekontrakt_df_cleaned = servicekontrakt_df_cleaned[servicekontrakt_df_cleaned['End_year'] >= 2010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse the name of the bandel (e.g., 306 (Borlänge)-Repbäcken) to extract the bandel number (e.g., 306) if any (sometimes not included), and the bandel name which must be included ((Borlänge)-Repbäcken)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'Bandel' column into two new columns 'Bandelnr' and 'Bandelnamn'\n",
    "def parse_bandel(bandel):\n",
    "    import re\n",
    "    bandelnr_match = re.match(r'^(\\d+(?:/\\d+)*)', bandel)\n",
    "    if bandelnr_match:\n",
    "        bandelnr = bandelnr_match.group(0).replace('/', ', ')\n",
    "        bandelnamn = bandel[len(bandelnr_match.group(0)):].strip()\n",
    "    else:\n",
    "        bandelnr = ''\n",
    "        bandelnamn = bandel.strip()\n",
    "    return pd.Series([bandelnr, bandelnamn])\n",
    "\n",
    "# Apply the parsing function to create two new columns\n",
    "servicekontrakt_df_processed = servicekontrakt_df_cleaned.copy()\n",
    "# in bandel, replace '–' with '-'\n",
    "servicekontrakt_df_processed['Bandel'] = servicekontrakt_df_processed['Bandel'].str.replace('–', '-')\n",
    "servicekontrakt_df_processed[['Bandelnr', 'Bandelnamn']] = servicekontrakt_df_processed['Bandel'].apply(parse_bandel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and cleaning BIS dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a BIS dictionary to find missing Bandelnr for some contracts (and later to construct a graph and find the length of shortest path). We first read the BIS dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and sheet details\n",
    "excel_file_path = \"../Python matching/raw_data/BIS-data 2024-01-09 - Bandel, plats och förbindelselinje, alla spår.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "bis_df = pd.read_excel(excel_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to clean up the dictionary by focusing on main tracks (nhsp) and removing duplicates, to speed up the matchning/search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding the code of the stations\n",
    "bis_df_no_duplicates = bis_df[['BdlNr', 'Bandel', 'Plats_sign', 'Plats']].drop_duplicates()\n",
    "# remove all rows with mising values of 'Plats_sign'\n",
    "bis_df_no_duplicates = bis_df_no_duplicates[bis_df_no_duplicates['Plats_sign'].notna()]\n",
    "\n",
    "# Focus on the main tracks\n",
    "# remove all rows where BdlNr is equal to 1 (Ingår ej i bandelsindelning) but not rows where Forbind is not empty (helpful for finding shortest path)\n",
    "bis_df = bis_df[(bis_df['BdlNr'] != 1) | (bis_df['Forbind'].notna())]\n",
    "\n",
    "# keep only rows where column Spår_huvud_sido is nhsp\n",
    "bis_df_nhsp = bis_df[bis_df[\"Spår_huvud_sido\"] == \"nhsp\"]\n",
    "\n",
    "# Step 1: Remove duplicates from the mapping\n",
    "bis_df_nhsp_no_duplicates = bis_df_nhsp[['BdlNr', 'Bandel', 'Plats_sign', 'Plats', 'Forbind']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also reorganize the same dataframe in order to easily get the length of the track section (Banlangd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'BdlNr', 'Bandel', 'Plats_sign', 'Plats' and sum 'Banlangd'\n",
    "grouped_by_plats = bis_df_nhsp.groupby(['BdlNr', 'Bandel', 'Plats_sign', 'Plats'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 2: Group by 'BdlNr', 'Bandel', 'Forbind' and sum 'Banlangd'\n",
    "grouped_by_forbind = bis_df_nhsp.groupby(['BdlNr', 'Bandel', 'Forbind'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 3: Add 'Plats_sign' and 'Plats' columns with NaN to 'grouped_by_forbind' for consistency\n",
    "grouped_by_forbind['Plats_sign'] = pd.NA\n",
    "grouped_by_forbind['Plats'] = pd.NA\n",
    "\n",
    "# Step 4: Add 'Forbind' column with NaN to 'grouped_by_plats' for consistency\n",
    "grouped_by_plats['Forbind'] = pd.NA\n",
    "\n",
    "# Step 5: Combine the two DataFrames using outer concatenation\n",
    "combined_bis_df_nhsp_langd = pd.concat([grouped_by_plats, grouped_by_forbind], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding missing Bandelnr (using dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, some rows do not have bandelnr (e.g., (Kävlinge) - (Arlöv)), we use the dictionary (from BIS) that we just prepared to find the missing Bandelnr. Bandelnr is important after all to be able to match to a common contract region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "# add column Bandelnamn_from_fuzzy_match to servicekontrakt_df_processed\n",
    "servicekontrakt_df_processed['Bandelnamn_from_fuzzy_match'] = None\n",
    "\n",
    "# Iterate over rows to find and update missing 'Bandelnr'\n",
    "for index, row in servicekontrakt_df_processed.iterrows():\n",
    "    if row['Bandelnr'] == \"\" and \"-\" in row['Bandelnamn']: # single stations are treated in the code just after\n",
    "        # Direct match in dictionary\n",
    "        bandel_nr = bis_df_no_duplicates[bis_df_no_duplicates['Bandel'] == row['Bandelnamn']]['BdlNr']\n",
    "        if len(bandel_nr) > 0:\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr'] = bandel_nr.values[0]\n",
    "            # put in a new column called Bandelnr_from_exact_match\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr_from_exact_match'] = bandel_nr.values[0]\n",
    "        else:\n",
    "            # Flexible matching using difflib\n",
    "            all_bandels = bis_df_no_duplicates['Bandel'].tolist()\n",
    "            closest_match = difflib.get_close_matches(row['Bandelnamn'], all_bandels, n=1, cutoff=0.8)\n",
    "            \n",
    "            if closest_match:\n",
    "                # Find the closest match's Bandelnr\n",
    "                bandel_nr = bis_df_no_duplicates[bis_df_no_duplicates['Bandel'] == closest_match[0]]['BdlNr']\n",
    "                if len(bandel_nr) > 0:\n",
    "                    servicekontrakt_df_processed.at[index, 'Bandelnr'] = bandel_nr.values[0]\n",
    "                    # put in a new column called Bandelnr_from_fuzzy_match\n",
    "                    servicekontrakt_df_processed.at[index, 'Bandelnr_from_fuzzy_match'] = bandel_nr.values[0]\n",
    "                    # put in a new column called Bandelnamn_from_fuzzy_match\n",
    "                    servicekontrakt_df_processed.at[index, 'Bandelnamn_from_fuzzy_match'] = closest_match[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some bandelnamn have a specific name of a place. For these, we can identify the bandelnr using the dictionary directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No match for Bandelnamn:  Helsingborg c\n",
      "No match for Bandelnamn:  Helsingborg c/ Helsingborg gbg\n",
      "No match for Bandelnamn:  Helsingborg gbg\n",
      "No match for Bandelnamn:  Landskrona Ö\n"
     ]
    }
   ],
   "source": [
    "# for the rows where Bandelnr is missing and Bandelnamn has no \"-\" in it, we find the bandelnr using bis_df [\"Plats\"]\n",
    "for index, row in servicekontrakt_df_processed.iterrows():\n",
    "    if row['Bandelnr'] == \"\" and \"-\" not in row['Bandelnamn']:\n",
    "        # Direct match in dictionary\n",
    "        bandel_nr = bis_df_nhsp_no_duplicates[bis_df_nhsp_no_duplicates['Plats'] == row['Bandelnamn']]['BdlNr']\n",
    "        if len(bandel_nr) == 1:\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr'] = bandel_nr.values[0]\n",
    "            # put in a new column called Bandelnr_from_exact_Plats_match\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr_from_exact_Plats_match'] = bandel_nr.values[0]\n",
    "        elif len(bandel_nr) > 1:\n",
    "            # save the first and print that there are multiple matches\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr'] = bandel_nr.values[0]\n",
    "            # put in a new column called Bandelnr_from_exact_Plats_match\n",
    "            servicekontrakt_df_processed.at[index, 'Bandelnr_from_exact_Plats_match'] = bandel_nr.values[0]\n",
    "            print(\"Multiple matches for Bandelnamn: \", row['Bandelnamn'])\n",
    "        else:\n",
    "            # print that there is no match\n",
    "            print(\"No match for Bandelnamn: \", row['Bandelnamn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Bandelnamn to Plats_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from Plats (full name) to Plats_sign (short code)\n",
    "name_to_code_mapping = bis_df_no_duplicates.set_index('Plats_sign')['Plats'].to_dict()\n",
    "\n",
    "def convert_bandelnamn_to_codes(bandelnamn):\n",
    "    # Split by dash and preserve parentheses\n",
    "    stations = bandelnamn.split('-')\n",
    "    \n",
    "    # Create a case-insensitive mapping of full names to codes\n",
    "    name_to_code_mapping_lower = {\n",
    "        str(v).lower(): str(k) for k, v in name_to_code_mapping.items()\n",
    "    }\n",
    "    \n",
    "    # Detailed conversion with original names preserved\n",
    "    station_details = []\n",
    "    station_codes = []\n",
    "\n",
    "    for name in stations:\n",
    "        stripped_name = str(name).strip()\n",
    "        has_parentheses = stripped_name.startswith('(') and stripped_name.endswith(')')\n",
    "        \n",
    "        # Remove parentheses temporarily for lookup\n",
    "        name_without_parentheses = stripped_name[1:-1] if has_parentheses else stripped_name\n",
    "        \n",
    "        # Try case-insensitive matching with original name\n",
    "        code = name_to_code_mapping_lower.get(name_without_parentheses.lower())\n",
    "        \n",
    "\n",
    "        # If no code found, try appending \" central\"\n",
    "        central_name = \"\"\n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses} central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses}s central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # trying appending \" c\"\n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses} c\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has \"gbg\" replace with godsbangård\n",
    "        if code is None and \" c\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" c\", \"s central\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has / then remove it the part after /\n",
    "        if code is None and \"/\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.split('/')[0].strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "            if code is None:\n",
    "                central_name = central_name.replace(\" c\", \"s central\").strip()\n",
    "                code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has \"gbg\" replace with godsbangård\n",
    "        if code is None and \" Ö\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" Ö\", \" Östra\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has \"gbg\" replace with godsbangård\n",
    "        if code is None and \"gbg\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" gbg\", \"s godsbangård\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        if code is None and \"Åby\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\"Åby\", \"Åby södra\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        if code is None and \" (Mssb)\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" (Mssb)\", \"\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        if code is None and \" V\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" V\", \"s västra\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "\n",
    "        # if word has \"rbg\" replace with rangerbangård\n",
    "        if code is None and \"rbg\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" rbg\", \"s rangerbangård\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has \"rbg\" replace with s central\n",
    "        if code is None and \"rbg\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" rbg\", \"s central\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # if word has \"N\" replace with norra\n",
    "        if code is None and \" N\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\" N\", \" norra\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "\n",
    "        # try removing \"driftsplats\" from the name\n",
    "        if code is None and \"driftsplats\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\"driftsplats\", \"\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "        # try removing \"driftsplats\" from the name\n",
    "        if code is None and \"C driftsplats\" in name_without_parentheses:\n",
    "            central_name = name_without_parentheses.replace(\"C driftsplats\", \"central\").strip()\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "\n",
    "\n",
    "        # if no code found, check if name_without_parentheses is composed of two names separated by a space and replace the space with a hyphen\n",
    "        if code is None:\n",
    "            names = name_without_parentheses.split(' ')\n",
    "            if len(names) == 2:\n",
    "                name_without_parentheses = names[0] + '-' + names[1]\n",
    "                code = name_to_code_mapping_lower.get(name_without_parentheses.lower(), None)\n",
    "\n",
    "        # if not found, try fuzzy matching\n",
    "        if code is None:\n",
    "            all_plats = bis_df_no_duplicates['Plats'].tolist()\n",
    "            closest_match = difflib.get_close_matches(name_without_parentheses, all_plats, n=1, cutoff=0.9)\n",
    "            if closest_match:\n",
    "                code = name_to_code_mapping_lower.get(closest_match[0].lower())\n",
    "        \n",
    "        station_details.append({\n",
    "            'original_name': stripped_name,\n",
    "            'tried_name': central_name if code and code != name_without_parentheses else None,\n",
    "            'station_code': code\n",
    "        })\n",
    "        \n",
    "        if code:\n",
    "            # Add parentheses back if they were present\n",
    "            formatted_code = f\"({code})\" if has_parentheses else code\n",
    "            station_codes.append(formatted_code)\n",
    "    \n",
    "    return {\n",
    "        'station_details': station_details,\n",
    "        'station_codes': station_codes,\n",
    "        'short_path': '-'.join(station_codes) if station_codes else None\n",
    "    }\n",
    "\n",
    "# Step 2: Prepare the dataframe with detailed conversion results\n",
    "def prepare_bandelnamn_conversion(servicekontrakt_df):\n",
    "    # Apply the conversion function\n",
    "    conversion_results = servicekontrakt_df['Bandelnamn'].apply(convert_bandelnamn_to_codes)\n",
    "    \n",
    "    # Extract details into separate columns\n",
    "    servicekontrakt_df = servicekontrakt_df.copy()\n",
    "    servicekontrakt_df['station_details'] = conversion_results.apply(lambda x: x['station_details'])\n",
    "    servicekontrakt_df['original_station_names'] = servicekontrakt_df['station_details'].apply(\n",
    "        lambda x: [detail['original_name'] for detail in x]\n",
    "    )\n",
    "    servicekontrakt_df['station_codes'] = conversion_results.apply(lambda x: x['station_codes'])\n",
    "    servicekontrakt_df['short_path'] = conversion_results.apply(lambda x: x['short_path'])\n",
    "    \n",
    "    return servicekontrakt_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the steps\n",
    "# 1. First, prepare the conversion\n",
    "servicekontrakt_df_processed = prepare_bandelnamn_conversion(servicekontrakt_df_processed)\n",
    "\n",
    "# # add a column named diff_len where you put the difference between the length of the list in original_station_names and station_codes\n",
    "# servicekontrakt_df_processed['diff_len'] = servicekontrakt_df_processed.apply(lambda x: len(x['original_station_names']) - len(x['station_codes']), axis=1)\n",
    "\n",
    "# # add another column with length of original_station_names\n",
    "# servicekontrakt_df_processed['len_original_station_names'] = servicekontrakt_df_processed.apply(lambda x: len(x['original_station_names']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows have have the same values of all the columns because they are belonging to different track. We remove these duplicates since we focus only on main track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns station_details, original_station_names and station_codes\n",
    "servicekontrakt_df_processed = servicekontrakt_df_processed.drop(columns=['station_details', 'original_station_names', 'station_codes'])\n",
    "\n",
    "# make sure bandelnr is a number integer\n",
    "servicekontrakt_df_processed['Bandelnr'] = pd.to_numeric(servicekontrakt_df_processed['Bandelnr'], errors='coerce')\n",
    "\n",
    "# remove duplicate rows from servicekontrakt_df_processed\n",
    "servicekontrakt_df_processed_no_duplicates = servicekontrakt_df_processed.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Banlangd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and easiest rows are the ones where we already have found exact/fuzzy matches (with bandelnamn) from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "servicekontrakt_df_langd = servicekontrakt_df_processed_no_duplicates.copy()\n",
    "\n",
    "# add a new column named 'Banlangd' to servicekontrakt_df_langd\n",
    "servicekontrakt_df_langd['Banlangd'] = pd.NA\n",
    "\n",
    "# iterate over rows in servicekontrakt_df_langd and find the corresponding Banlangd in combined_bis_df_nhsp_langd\n",
    "# use the columns 'Bandelnr' and 'Bandelnamn_from_fuzzy_match' to find the corresponding Banlangd by summing up the Banlangd values in combined_bis_df_nhsp_langd\n",
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    # if bandelnamn_from_fuzzy_match column is in row \n",
    "    if 'Bandelnamn_from_fuzzy_match' in row and not pd.isna(row['Bandelnamn_from_fuzzy_match']):    \n",
    "        bandel_namn_exakt = row['Bandelnamn_from_fuzzy_match']\n",
    "        bandel_nr = row['Bandelnr']\n",
    "        # find the corresponding Banlangd in combined_bis_df_nhsp_langd\n",
    "        banlangd = combined_bis_df_nhsp_langd[(combined_bis_df_nhsp_langd['BdlNr'] == bandel_nr) & (combined_bis_df_nhsp_langd['Bandel'] == bandel_namn_exakt)]['Banlangd']\n",
    "        if len(banlangd) > 0:\n",
    "            # sum up the Banlangd values\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = banlangd.sum()/1000\n",
    "        else:\n",
    "            # put zero if no Banlangd found\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = 0\n",
    "    # do the same for Bandelnr_from_exact_match\n",
    "    elif 'Bandelnr_from_exact_match' in row and not pd.isna(row['Bandelnr_from_exact_match']):\n",
    "        bandel_namn_exakt = row['Bandelnamn']\n",
    "        bandel_nr = row['Bandelnr']\n",
    "        # find the corresponding Banlangd in combined_bis_df_nhsp_langd\n",
    "        banlangd = combined_bis_df_nhsp_langd[(combined_bis_df_nhsp_langd['BdlNr'] == bandel_nr) & (combined_bis_df_nhsp_langd['Bandel'] == bandel_namn_exakt)]['Banlangd']\n",
    "        if len(banlangd) > 0:\n",
    "            # sum up the Banlangd values\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = banlangd.sum()/1000\n",
    "        else:\n",
    "            # put zero if no Banlangd found\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = 0\n",
    "    # do the same for Bandelnr_from_exact_Plats_match\n",
    "    elif 'Bandelnr_from_exact_Plats_match' in row and not pd.isna(row['Bandelnr_from_exact_Plats_match']):\n",
    "        bandel_namn_exakt = row['Bandelnamn']\n",
    "        bandel_nr = row['Bandelnr']\n",
    "        # find the corresponding Banlangd in combined_bis_df_nhsp_langd\n",
    "        banlangd = combined_bis_df_nhsp_langd[(combined_bis_df_nhsp_langd['BdlNr'] == bandel_nr) & (combined_bis_df_nhsp_langd['Plats'] == bandel_namn_exakt)]['Banlangd']\n",
    "        if len(banlangd) > 0:\n",
    "            # sum up the Banlangd values\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = banlangd.sum()/1000\n",
    "        else:\n",
    "            # put zero if no Banlangd found\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of the column Bandelnamn_from_fuzzy_match, Bandelnr_from_exact_match and Bandelnr_from_exact_Plats_match if they exist\n",
    "if 'Bandelnamn_from_fuzzy_match' in servicekontrakt_df_langd:\n",
    "    servicekontrakt_df_langd = servicekontrakt_df_langd.drop(columns=['Bandelnamn_from_fuzzy_match'])\n",
    "if 'Bandelnr_from_exact_match' in servicekontrakt_df_langd:\n",
    "    servicekontrakt_df_langd = servicekontrakt_df_langd.drop(columns=['Bandelnr_from_exact_match'])\n",
    "if 'Bandelnr_from_exact_Plats_match' in servicekontrakt_df_langd:\n",
    "    servicekontrakt_df_langd = servicekontrakt_df_langd.drop(columns=['Bandelnr_from_exact_Plats_match'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next easiest are the rows which correspond to a single station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over rows in servicekontrakt_df_langd (with missing values of langd and Bandelnamn without \"-\")\n",
    "# and find the corresponding Banlangd using short_path = combined_bis_df_nhsp_langd['Plats']\n",
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    if pd.isna(row['Banlangd']):\n",
    "        # if Bandelnamn has \"-\" just continue\n",
    "        if \"-\" in row['Bandelnamn']:\n",
    "            continue\n",
    "        # if not find the corresponding Banlangd using short_path\n",
    "        bandel_nr = row['Bandelnr']\n",
    "        driftplats_sign = row['short_path']\n",
    "        # find the corresponding Banlangd in combined_bis_df_nhsp_langd\n",
    "        banlangd = combined_bis_df_nhsp_langd[(combined_bis_df_nhsp_langd['BdlNr'] == bandel_nr) & (combined_bis_df_nhsp_langd['Plats_sign'] == driftplats_sign)]['Banlangd']\n",
    "        if len(banlangd) > 0:\n",
    "            # sum up the Banlangd values\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = banlangd.sum()/1000 \n",
    "        else:\n",
    "            # put zero if no Banlangd found\n",
    "            servicekontrakt_df_langd.at[index, 'Banlangd'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the length for the rows, we will need to construct a graph of the network and find the shortest path and then accumulate the length on the main tracks if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'BdlNr', 'Bandel', 'Plats_sign', 'Plats' and sum 'Banlangd' where 'Spår_huvud_sido' is 'nhsp'\n",
    "grouped_by_plats_nhsp = bis_df[bis_df['Spår_huvud_sido'] == 'nhsp'].groupby(['BdlNr', 'Bandel', 'Plats_sign', 'Plats'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 2: Group by 'BdlNr', 'Bandel', 'Forbind' and sum 'Banlangd' where 'Spår_huvud_sido' is 'nhsp'\n",
    "grouped_by_forbind_nhsp = bis_df[bis_df['Spår_huvud_sido'] == 'nhsp'].groupby(['BdlNr', 'Bandel', 'Forbind'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 3: Group by 'BdlNr', 'Bandel', 'Plats_sign', 'Plats' where 'Spår_huvud_sido' is not 'nhsp' and set 'Banlangd' to 0\n",
    "grouped_by_plats_non_nhsp = bis_df[bis_df['Spår_huvud_sido'] != 'nhsp'].groupby(['BdlNr', 'Bandel', 'Plats_sign', 'Plats']).size().reset_index(name='Banlangd')\n",
    "grouped_by_plats_non_nhsp['Banlangd'] = 0\n",
    "\n",
    "# Step 4: Group by 'BdlNr', 'Bandel', 'Forbind' where 'Spår_huvud_sido' is not 'nhsp' and set 'Banlangd' to 0\n",
    "grouped_by_forbind_non_nhsp = bis_df[bis_df['Spår_huvud_sido'] != 'nhsp'].groupby(['BdlNr', 'Bandel', 'Forbind']).size().reset_index(name='Banlangd')\n",
    "grouped_by_forbind_non_nhsp['Banlangd'] = 0\n",
    "\n",
    "# Combine nhsp and non-nhsp dataframes\n",
    "grouped_by_plats = pd.concat([grouped_by_plats_nhsp, grouped_by_plats_non_nhsp], ignore_index=True)\n",
    "grouped_by_forbind = pd.concat([grouped_by_forbind_nhsp, grouped_by_forbind_non_nhsp], ignore_index=True)\n",
    "\n",
    "# when when duplicate rows are present, keep the none with the highest Banlangd\n",
    "grouped_by_plats = grouped_by_plats.sort_values('Banlangd', ascending=False).drop_duplicates(['BdlNr', 'Bandel', 'Plats_sign', 'Plats']).sort_index()\n",
    "grouped_by_forbind = grouped_by_forbind.sort_values('Banlangd', ascending=False).drop_duplicates(['BdlNr', 'Bandel', 'Forbind']).sort_index()\n",
    "\n",
    "# Step 3: Add 'Plats_sign' and 'Plats' columns with NaN to 'grouped_by_forbind' for consistency\n",
    "grouped_by_forbind['Plats_sign'] = pd.NA\n",
    "grouped_by_forbind['Plats'] = pd.NA\n",
    "\n",
    "# Step 4: Add 'Forbind' column with NaN to 'grouped_by_plats' for consistency\n",
    "grouped_by_plats['Forbind'] = pd.NA\n",
    "\n",
    "# Step 5: Combine the two DataFrames using outer concatenation\n",
    "combined_bis_df_langd = pd.concat([grouped_by_plats, grouped_by_forbind], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Global cache for lengths and the graph\n",
    "langd_cache = {}\n",
    "GLOBAL_GRAPH = None\n",
    "\n",
    "# Step 1: Create a mapping from Plats_sign (full name) to Banlangd\n",
    "station_length_lookup = combined_bis_df_langd.set_index('Plats_sign')['Banlangd'].to_dict()\n",
    "\n",
    "### Utility Functions ###\n",
    "\n",
    "def initialize_global_graph(dictionary_df):\n",
    "    \"\"\"Initialize the global graph once\"\"\"\n",
    "    global GLOBAL_GRAPH\n",
    "    if GLOBAL_GRAPH is None:\n",
    "        #bdl_df = dictionary_df[(dictionary_df['BdlNr'] >= 2) & (dictionary_df['BdlNr'] <= 990)]\n",
    "        bdl_df = dictionary_df\n",
    "        GLOBAL_GRAPH = nx.Graph()  # Undirected graph to simulate bidirectional connections\n",
    "        for _, row in bdl_df.iterrows():\n",
    "            if pd.notna(row['Forbind']):\n",
    "                start, end = row['Forbind'].split('-')\n",
    "                length = row['Banlangd']\n",
    "                GLOBAL_GRAPH.add_edge(start.strip(), end.strip(), length=length)\n",
    "\n",
    "def calculate_sum_langd(forbind_list, dictionary_df):\n",
    "    if not forbind_list or forbind_list == '':\n",
    "        # print that the forbind_list is empty\n",
    "        print(\"forbind_list is empty\")\n",
    "        return None\n",
    "    \n",
    "    # Check cache\n",
    "    cache_key = (forbind_list)\n",
    "    if cache_key in langd_cache:\n",
    "        return langd_cache[cache_key]\n",
    "    \n",
    "    # Initialize global graph if not already done\n",
    "    if GLOBAL_GRAPH is None:\n",
    "        initialize_global_graph(dictionary_df)\n",
    "    \n",
    "    # Split and clean the forbind_list\n",
    "    forbinds = [f.strip() for f in forbind_list.split(',')]\n",
    "    stations = [station for forbind in forbinds for station in forbind.split('-')]\n",
    "    first_station = stations[0]\n",
    "    last_station = stations[-1]\n",
    "    \n",
    "    # Check if first and last stations are enclosed in parentheses\n",
    "    include_first_station = not (first_station.startswith('(') and first_station.endswith(')'))\n",
    "    include_last_station = not (last_station.startswith('(') and last_station.endswith(')'))\n",
    "    \n",
    "    # Remove parentheses for lookup in the graph\n",
    "    first_station_cleaned = first_station.strip('()')\n",
    "    last_station_cleaned = last_station.strip('()')\n",
    "    \n",
    "    # Check if stations exist in graph\n",
    "    if first_station_cleaned not in GLOBAL_GRAPH or last_station_cleaned not in GLOBAL_GRAPH:\n",
    "        langd_cache[cache_key] = None\n",
    "        print(\"Stations not found in graph\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path_length = nx.shortest_path_length(\n",
    "            GLOBAL_GRAPH, \n",
    "            source=first_station_cleaned, \n",
    "            target=last_station_cleaned, \n",
    "            weight='length'\n",
    "        )\n",
    "        \n",
    "        # Calculate length of intermediate stations\n",
    "        shortest_path_stations = nx.shortest_path(\n",
    "            GLOBAL_GRAPH, \n",
    "            source=first_station_cleaned, \n",
    "            target=last_station_cleaned\n",
    "        )\n",
    "        intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "        station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "        \n",
    "        # Add lengths of first and last stations based on inclusion rules\n",
    "        if include_first_station:\n",
    "            station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "        if include_last_station:\n",
    "            station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "        total_length = path_length + station_length_sum\n",
    "        langd_cache[cache_key] = total_length\n",
    "        return total_length\n",
    "        \n",
    "    except nx.NetworkXNoPath:\n",
    "        langd_cache[cache_key] = None\n",
    "        return None  # No path found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Banlanged (from the shortest path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_langd_for_bandelnamn(row, dictionary_df):\n",
    "\n",
    "    row_bandel = None\n",
    "    row_forbind = None\n",
    "    if pd.notna(row['Bandelnr']):\n",
    "        row_bandel = int(row['Bandelnr'])\n",
    "    row_forbind = row['short_path']\n",
    "\n",
    "    # Case 1: Single station\n",
    "    if '-' not in row_forbind:\n",
    "        single_station = row_forbind\n",
    "        \n",
    "        # Find the row in dictionary_df where Plats_sign matches the single station\n",
    "        matching_station = dictionary_df[dictionary_df['Plats_sign'] == single_station]\n",
    "        \n",
    "    \n",
    "        # keep only rows where BdlNr is same as row['Bandelnr']\n",
    "        matching_station = matching_station[matching_station['BdlNr'] == row_bandel]\n",
    "\n",
    "        # If no matching station is found, return None\n",
    "        if matching_station.empty:\n",
    "            print(f\"No matching station found for {single_station}\")\n",
    "            return None\n",
    "        \n",
    "        # Get the station's length as the sum of all values in matching_station['Banlangd']\n",
    "        # station_length = matching_station['Banlangd'].iloc[0]\n",
    "        station_length = matching_station['Banlangd'].sum()\n",
    "        \n",
    "        return station_length/1000  # Convert to kilometers\n",
    "\n",
    "    # Case 2: Multiple stations (existing logic)\n",
    "    return calculate_sum_langd(row_forbind, dictionary_df)/1000  # Convert to kilometers\n",
    "\n",
    "# remove rows from dictionary_df where BdlNr is 1\n",
    "#dictionary_df = dictionary_df[dictionary_df['BdlNr'] != 1]\n",
    "\n",
    "# 2. Then calculate sum_langd\n",
    "# call calculate_sum_langd_for_bandelnamn to calculate the langd for rows where Langd is missing\n",
    "# servicekontrakt_df_langd['sum_langd'] = servicekontrakt_df_langd.apply(\n",
    "#     lambda row: calculate_sum_langd_for_bandelnamn(row, combined_bis_df_langd),\n",
    "#     axis=1\n",
    "# )\n",
    "servicekontrakt_df_langd.loc[servicekontrakt_df_langd['Banlangd'].isna(), 'Banlangd'] = servicekontrakt_df_langd[servicekontrakt_df_langd['Banlangd'].isna()].apply(\n",
    "    lambda row: calculate_sum_langd_for_bandelnamn(row, combined_bis_df_langd),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# rename the column 'Banlangd' to 'sum_langd'\n",
    "servicekontrakt_df_langd = servicekontrakt_df_langd.rename(columns={'Banlangd': 'sum_langd'})\n",
    "\n",
    "# make sure column sum_langd is a real number\n",
    "servicekontrakt_df_langd['sum_langd'] = pd.to_numeric(servicekontrakt_df_langd['sum_langd'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add track times in km-hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have caclulated the lengths of each track segment, we can now add columns for track access times in km-hours for the following columns:\n",
    "- 'TPA timmar per år',\n",
    "- 'TPA timmar natt per år'\n",
    "- 'TPA timmar helg per år', \n",
    "- 'EJ TPA timmar per år'\n",
    "- 'EJ TPA timmar natt per år'\n",
    "- 'EJ TPA timmar helg per år'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to calculate track access times (km-hours)\n",
    "track_time_columns = [\n",
    "    'TPA timmar per år', 'TPA timmar natt per år', 'TPA timmar helg per år', \n",
    "    'EJ TPA timmar per år', 'EJ TPA timmar natt per år', 'EJ TPA timmar helg per år'\n",
    "]\n",
    "\n",
    "# Create new columns for km-hours by multiplying each track time column by 'sum_langd'\n",
    "for col in track_time_columns:\n",
    "    km_hour_col = col.replace('timmar', 'km-timmar') # Naming the new column\n",
    "    servicekontrakt_df_langd[km_hour_col] = servicekontrakt_df_langd[col] * servicekontrakt_df_langd['sum_langd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching to contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading BIS-contract file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the excel file containing BIS information for mapping the bandel number with contract name. The file name is BIS_24_kontrakt_bandel_plats.xlsx and has sheet BIS 2024-01-09 with columns such as Bandel_nummer, UH_kontraktsområde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and sheet details\n",
    "excel_file_path = \"../Python matching/raw_data/BIS_24_kontrakt_bandel_plats.xlsx\"\n",
    "sheet_name = \"BIS 2024-01-09\"\n",
    "\n",
    "# Load the Excel file\n",
    "bis_kontrakt_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing mapping function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the mapping (bandel_nummer, Plats <-> UH kontraktområde)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove duplicates from the mapping\n",
    "bandel_plats_to_contract_map = bis_kontrakt_df[['Bandel_nummer', 'Plats_sign','UH_kontraktsområde']].drop_duplicates()\n",
    "\n",
    "# Step 2: Filter out rows where UH_kontraktsområde is NaN or 'Ingår inte i något kontrakt'\n",
    "bandel_plats_to_contract_map = bandel_plats_to_contract_map[\n",
    "    bandel_plats_to_contract_map['UH_kontraktsområde'].notna() & \n",
    "    (bandel_plats_to_contract_map['UH_kontraktsområde'] != 'Ingår inte i något kontrakt')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we create a new column and fill it progressively with matchings. We start with the case where both Bandelnr and single station is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty column kontrakt_från_bandel in servicekontrakt_df_langd\n",
    "servicekontrakt_df_langd['kontrakt_från_bandel'] = pd.NA\n",
    "\n",
    "# for rows with non-empty Bandelnr, and short_path not containing \"-\" find the corresponding kontrakt_från_bandel\n",
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    if pd.notna(row['Bandelnr']) and '-' not in row['short_path']:\n",
    "        bandel_nr = int(row['Bandelnr'])\n",
    "        driftplats_sign = row['short_path']\n",
    "        # remove parentheses\n",
    "        driftplats_sign = driftplats_sign.strip('()')\n",
    "        # find the corresponding kontrakt_från_bandel in bandel_plats_to_contract_map\n",
    "        kontrakt = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Bandel_nummer'] == bandel_nr) & (bandel_plats_to_contract_map['Plats_sign'] == driftplats_sign)]['UH_kontraktsområde']\n",
    "        if len(kontrakt) == 1:\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt.values[0]\n",
    "        elif len(kontrakt) > 1:\n",
    "            # save the first and print that there are multiple matches\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt.values[0]\n",
    "            print(\"Multiple matches for Bandelnr: \", bandel_nr)\n",
    "        else:\n",
    "            # put zero if no kontrakt_från_bandel found\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = 'Inget kontrakt'\n",
    "            print(\"No match for Bandelnr: \", bandel_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We move on to map rows where we have two stations and exactly one of them does not have parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will map rows with missing kontrakt_från_bandel that have Bandelnamn with \"-\" in it and no two occurrence of \"(\"  in short_path\n",
    "# we split the short_path by \"-\" and then take element wihout parentheses and find the corresponding kontrakt_från_bandel\n",
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    if '-' in row['short_path'] and row['short_path'].count('(') == 1:\n",
    "        # split the short_path by \"-\"\n",
    "        stations = row['short_path'].split('-')\n",
    "        if stations[0].startswith('('):\n",
    "            station_sign = stations[1]\n",
    "            kontrakt = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == station_sign)]['UH_kontraktsområde'].unique()\n",
    "        else:\n",
    "            station_sign = stations[0]\n",
    "            kontrakt = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == station_sign)]['UH_kontraktsområde'].unique()\n",
    "        if len(kontrakt) == 1:\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "        elif len(kontrakt) > 1:\n",
    "            if pd.notna(row['Bandelnr']):\n",
    "                bandel_nr = int(row['Bandelnr'])\n",
    "                kontrakt = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Bandel_nummer'] == bandel_nr) & (bandel_plats_to_contract_map['Plats_sign'] == station_sign)]['UH_kontraktsområde'].unique()\n",
    "                if len(kontrakt) == 1:\n",
    "                    servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "                elif len(kontrakt) > 1:\n",
    "                    # save the first and print that there are multiple matches\n",
    "                    servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "                    print(\"Multiple matches  (even after fixing bandelnr) for Bandelnr: \", bandel_nr)\n",
    "            else:\n",
    "                # save the first and print that there are multiple matches\n",
    "                servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "                print(\"Multiple matches for Bandelnamn: \", station_sign)\n",
    "        else:\n",
    "            # put zero if no kontrakt_från_bandel found\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = 'Inget kontrakt'\n",
    "            print(\"No match for Bandelnamn: \", station_sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, are the rows where no parentheses exist in the track section (i.e., both ends are included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple matches for Bandelnamn: Vns and Thö with Kontraktsområdesnamn: Långsele-Vännäs, Botniabanan. Closest match: ['Långsele-Vännäs inkl Botniabanan o Forsmo-Hoting']\n",
      "Multiple matches for Bandelnamn: Ap and Lsl with Kontraktsområdesnamn: Långsele-Vännäs, Botniabanan. Closest match: ['Långsele-Vännäs inkl Botniabanan o Forsmo-Hoting']\n"
     ]
    }
   ],
   "source": [
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    if '-' in row['short_path'] and row['short_path'].count('(') == 0:\n",
    "        # split the short_path by \"-\"\n",
    "        stations = row['short_path'].split('-')\n",
    "        kontrakt_1 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[0])]['UH_kontraktsområde'].unique()\n",
    "        kontrakt_2 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[1])]['UH_kontraktsområde'].unique()\n",
    "        if pd.notna(row['Bandelnr']):\n",
    "            bandel_nr = int(row['Bandelnr'])\n",
    "            if len(kontrakt_1) > 1:\n",
    "                kontrakt_1 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Bandel_nummer'] == bandel_nr) & (bandel_plats_to_contract_map['Plats_sign'] == stations[0])]['UH_kontraktsområde'].unique()\n",
    "            if len(kontrakt_2) > 1:\n",
    "                kontrakt_2 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Bandel_nummer'] == bandel_nr) & (bandel_plats_to_contract_map['Plats_sign'] == stations[1])]['UH_kontraktsområde'].unique()\n",
    "        # kontrakt will be the union of kontrakt_1 and kontrakt_2\n",
    "        kontrakt = list(set(kontrakt_1) | set(kontrakt_2))\n",
    "        # keep only unique values\n",
    "        if len(kontrakt) == 1:\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "        elif len(kontrakt) > 1:\n",
    "            # of the elements in kontrakt find the closest match using fuzzy matching to row['Kontraktsområdesnamn']   \n",
    "            closest_match = difflib.get_close_matches(row['Kontraktsområdesnamn'], kontrakt, n=1, cutoff=0.6)     \n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = closest_match[0]\n",
    "            # print the value of the closest match as well as row['Kontraktsområdesnamn']\n",
    "            print(f\"Multiple matches for Bandelnamn: {stations[0]} and {stations[1]} with Kontraktsområdesnamn: {row['Kontraktsområdesnamn']}. Closest match: {closest_match}\")\n",
    "        else:\n",
    "            # put zero if no kontrakt_från_bandel found\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = 'Inget kontrakt'\n",
    "            print(\"No match for Bandelnamn: \", station_sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we match track sections where the ends are not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple matches for Bandelnamn: ['Stockholm Nord' 'Mälarbanan'] with Kontraktsområdesnamn: Mälarbanan. Closest match: ['Mälarbanan']\n",
      "Multiple matches for Bandelnamn: ['Norra stambanan', 'Ostkustbanan'] with Kontraktsområdesnamn: Norra Stambanan. Closest match: ['Norra stambanan']\n",
      "Multiple matches for Bandelnamn: ['Norra stambanan', 'Ostkustbanan'] with Kontraktsområdesnamn: Norra Stambanan. Closest match: ['Norra stambanan']\n",
      "Multiple matches for Bandelnamn: ['Ostkustbanan' 'Stockholm Nord'] with Kontraktsområdesnamn: Ostkustbanan. Closest match: ['Ostkustbanan']\n",
      "Multiple matches for Bandelnamn: ['Stockholm Nord', 'Stockholm Mitt Drift o avhjälpande/Förebyggand'] with Kontraktsområdesnamn: Stockholm Mitt. Closest match: ['Stockholm Mitt Drift o avhjälpande/Förebyggand']\n",
      "Multiple matches for Bandelnamn: ['Ostkustbanan' 'Stockholm Nord'] with Kontraktsområdesnamn: Stockholm Nord. Closest match: ['Stockholm Nord']\n",
      "Multiple matches for Bandelnamn: ['Västra Södra Stambanan' 'Stockholm Syd'] with Kontraktsområdesnamn: Stockholm Syd. Closest match: ['Stockholm Syd']\n",
      "Multiple matches for Bandelnamn: ['Stockholm Syd' 'Svealandsbanan'] with Kontraktsområdesnamn: Stockholm Syd. Closest match: ['Stockholm Syd']\n",
      "Multiple matches for Bandelnamn: ['Stockholm Syd' 'Svealandsbanan'] with Kontraktsområdesnamn: Svealandsbanan. Closest match: ['Svealandsbanan']\n",
      "Multiple matches for Bandelnamn: ['Västkustbanan syd', 'Södra stambanan 2, del 2'] with Kontraktsområdesnamn: Västkustbanan Syd. Closest match: ['Västkustbanan syd']\n",
      "Multiple matches for Bandelnamn: ['Västkustbanan syd', 'Södra stambanan 2, del 2'] with Kontraktsområdesnamn: Västkustbanan Syd. Closest match: ['Västkustbanan syd']\n",
      "Multiple matches for Bandelnamn: ['Västra Södra Stambanan' 'Stockholm Syd'] with Kontraktsområdesnamn: Västra Södra Stambanan (VSSB). Closest match: ['Västra Södra Stambanan']\n",
      "Multiple matches for Bandelnamn: ['Västra Södra Stambanan' 'Stockholm Syd'] with Kontraktsområdesnamn: Västra Södra Stambanan (VSSB). Closest match: ['Västra Södra Stambanan']\n"
     ]
    }
   ],
   "source": [
    "for index, row in servicekontrakt_df_langd.iterrows():\n",
    "    if '-' in row['short_path'] and row['short_path'].count('(') == 2:\n",
    "        kontrakt = []\n",
    "        if pd.notna(row['Bandelnr']):\n",
    "            bandel_nr = int(row['Bandelnr'])\n",
    "            kontrakt = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Bandel_nummer'] == bandel_nr)]['UH_kontraktsområde'].unique()\n",
    "            if len(kontrakt) == 1:\n",
    "                servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "                continue\n",
    "            elif len(kontrakt) > 1:\n",
    "                # of the elements in kontrakt find the closest match using fuzzy matching to row['Kontraktsområdesnamn'] \n",
    "                # check row['Kontraktsområdesnamn'] is a substring of an element in kontrakt, if yes then choose that element\n",
    "                kontrakt_substring = [x for x in kontrakt if row['Kontraktsområdesnamn'] in x]\n",
    "                if len(kontrakt_substring) == 1:\n",
    "                    servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt_substring[0]\n",
    "                    # print the value of the closest match as well as row['Kontraktsområdesnamn']\n",
    "                    print(f\"Multiple matches for Bandelnamn: {kontrakt} with Kontraktsområdesnamn: {row['Kontraktsområdesnamn']}. Closest match: {kontrakt_substring}\")\n",
    "                    continue\n",
    "                # if not found, try fuzzy matching\n",
    "                closest_match = difflib.get_close_matches(row['Kontraktsområdesnamn'], kontrakt, n=1, cutoff=0.7)  \n",
    "                servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = closest_match[0]\n",
    "                # print the value of the closest match as well as row['Kontraktsområdesnamn']\n",
    "                print(f\"Multiple matches for Bandelnamn: {kontrakt} with Kontraktsområdesnamn: {row['Kontraktsområdesnamn']}. Closest match: {closest_match}\")\n",
    "                continue\n",
    "        # split the short_path by \"-\"\n",
    "        stations = row['short_path'].split('-')\n",
    "        # remove parentheses\n",
    "        stations = [station.strip('()') for station in stations]\n",
    "        kontrakt_1 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[0])]['UH_kontraktsområde'].unique()\n",
    "        kontrakt_2 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[1])]['UH_kontraktsområde'].unique()\n",
    "        if len(kontrakt_1) > 1:\n",
    "            kontrakt_1 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[0])]['UH_kontraktsområde'].unique()\n",
    "        if len(kontrakt_2) > 1:\n",
    "            kontrakt_2 = bandel_plats_to_contract_map[(bandel_plats_to_contract_map['Plats_sign'] == stations[1])]['UH_kontraktsområde'].unique()\n",
    "        # kontrakt will be the union of kontrakt_1 and kontrakt_2\n",
    "        kontrakt = list(set(kontrakt_1) | set(kontrakt_2))\n",
    "        # keep only unique values\n",
    "        if len(kontrakt) == 1:\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt[0]\n",
    "        elif len(kontrakt) > 1:\n",
    "            # check row['Kontraktsområdesnamn'] is a substring of an element in kontrakt, if yes then choose that element\n",
    "            kontrakt_substring = [x for x in kontrakt if row['Kontraktsområdesnamn'] in x]\n",
    "            if len(kontrakt_substring) == 1:\n",
    "                servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = kontrakt_substring[0]\n",
    "                # print the value of the closest match as well as row['Kontraktsområdesnamn']\n",
    "                print(f\"Multiple matches for Bandelnamn: {kontrakt} with Kontraktsområdesnamn: {row['Kontraktsområdesnamn']}. Closest match: {kontrakt_substring}\")\n",
    "                continue\n",
    "            # of the elements in kontrakt find the closest match using fuzzy matching to row['Kontraktsområdesnamn']   \n",
    "            closest_match = difflib.get_close_matches(row['Kontraktsområdesnamn'], kontrakt, n=1, cutoff=0.7)     \n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = closest_match[0]\n",
    "            # print the value of the closest match as well as row['Kontraktsområdesnamn']\n",
    "            print(f\"Multiple matches for Bandelnamn: {kontrakt} with Kontraktsområdesnamn: {row['Kontraktsområdesnamn']}. Closest match: {closest_match}\")\n",
    "        else:\n",
    "            # put zero if no kontrakt_från_bandel found\n",
    "            servicekontrakt_df_langd.at[index, 'kontrakt_från_bandel'] = 'Inget kontrakt'\n",
    "            print(\"No match for Bandelnamn: \", station_sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export processed cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the relevant columns for visualisation in Power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column Total timmar per år by adding TPA timmar per år and EJ TPA timmar per år\n",
    "servicekontrakt_df_langd['Total timmar per år'] = servicekontrakt_df_langd['TPA timmar per år'] + servicekontrakt_df_langd['EJ TPA timmar per år']\n",
    "servicekontrakt_df_langd['Total km-timmar per år'] = servicekontrakt_df_langd['TPA km-timmar per år'] + servicekontrakt_df_langd['EJ TPA km-timmar per år']\n",
    "\n",
    "#  for excel\n",
    "excel_file_path = \"./exported_data_regression/Servicekontrakt_per_bandel_matched_all_2011_2023.xlsx\"\n",
    "# servicekontrakt_df_to_export = servicekontrakt_df_langd[['Kontraktsområdesnamn', 'kontrakt_från_bandel', 'Tidsperiod', 'Bandel', 'TPA timmar per år',\n",
    "#        'TPA dagar per år', 'TPA veckor per år', 'TPA timmar natt per år',\n",
    "#        'TPA timmar helg per år', 'EJ TPA timmar per år', 'EJ TPA dagar per år',\n",
    "#        'EJ TPA veckor per år', 'EJ TPA timmar natt per år',\n",
    "#        'EJ TPA timmar helg per år', 'Total timmar per år', 'Bandelnr',\n",
    "#        'Bandelnamn', 'sum_langd']]\n",
    "servicekontrakt_df_langd.to_excel(excel_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
