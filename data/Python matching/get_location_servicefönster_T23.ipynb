{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching the location of each TCR to contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by reading the cleaned and matched TCRs for 2024 which are matched with contracts number. As well as Karins matching between bandel and number of hours of servicefönster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Förbindelser and BIS (for mapping Bandelnr <-> Kontrakt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Excel file containing the dictionary for bandel matching, and keep only relevant columns and bandel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dictionary_file_path = \"Förbindelselinje_2023_alla.xlsx\"\n",
    "\n",
    "# Read the entire dictionary into a DataFrame\n",
    "dictionary_df = pd.read_excel(dictionary_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Group by 'BdlNr', 'Bandel', 'Plats_sign', 'Plats' and sum 'Banlangd'\n",
    "grouped_by_plats = dictionary_df.groupby(['BdlNr', 'Bandel', 'Plats_sign', 'Plats'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 2: Group by 'BdlNr', 'Bandel', 'Forbind' and sum 'Banlangd'\n",
    "grouped_by_forbind = dictionary_df.groupby(['BdlNr', 'Bandel', 'Forbind'])['Banlangd'].sum().reset_index()\n",
    "\n",
    "# Step 3: Add 'Plats_sign' and 'Plats' columns with NaN to 'grouped_by_forbind' for consistency\n",
    "grouped_by_forbind['Plats_sign'] = pd.NA\n",
    "grouped_by_forbind['Plats'] = pd.NA\n",
    "\n",
    "# Step 4: Add 'Forbind' column with NaN to 'grouped_by_plats' for consistency\n",
    "grouped_by_plats['Forbind'] = pd.NA\n",
    "\n",
    "# Step 5: Combine the two DataFrames using outer concatenation\n",
    "combined_df = pd.concat([grouped_by_plats, grouped_by_forbind], ignore_index=True, sort=False)\n",
    "\n",
    "# Step 6: Reorder columns for clarity\n",
    "dictionary_df = combined_df[['BdlNr', 'Bandel', 'Plats_sign', 'Plats', 'Forbind', 'Banlangd']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the excel file containing BIS information for mapping the bandel number with contract name. The file name is BIS_24_kontrakt_bandel_plats.xlsx and has sheet BIS 2024-01-09 with columns such as Bandel_nummer, UH_kontraktsområde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and sheet details\n",
    "excel_file_path = \"BIS_24_kontrakt_bandel_plats.xlsx\"\n",
    "sheet_name = \"BIS 2024-01-09\"\n",
    "\n",
    "# Load the Excel file\n",
    "bis_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Step 1: Remove duplicates from the mapping\n",
    "bandel_to_contract_map = bis_df[['Bandel_nummer', 'UH_kontraktsområde']].drop_duplicates()\n",
    "\n",
    "# Step 2: Filter out rows where UH_kontraktsområde is NaN or 'Ingår inte i något kontrakt'\n",
    "bandel_to_contract_map = bandel_to_contract_map[\n",
    "    bandel_to_contract_map['UH_kontraktsområde'].notna() & \n",
    "    (bandel_to_contract_map['UH_kontraktsområde'] != 'Ingår inte i något kontrakt')\n",
    "]\n",
    "\n",
    "# Convert to a dictionary for fast lookups\n",
    "bandel_contract_dict = bandel_to_contract_map.set_index('Bandel_nummer')['UH_kontraktsområde'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karin's bandelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Excel file containing service contracts for each bandel\n",
    "#excel_file_path = \"servicekontrakt_per_bandel_Abdou.xlsx\"\n",
    "excel_file_path = \"more_servicekontrakt_per_bandel.xlsx\"\n",
    "\n",
    "#sheet_name = \"uppdaterad\"\n",
    "sheet_name = \"tid per bandel\"\n",
    "\n",
    "# Read the specific sheet 'T24' into a DataFrame\n",
    "servicekontrakt_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where the third column is missing\n",
    "servicekontrakt_df = servicekontrakt_df[servicekontrakt_df.iloc[:, 2].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'Bandel' column into two new columns 'Bandelnr' and 'Bandelnamn'\n",
    "def parse_bandel(bandel):\n",
    "    import re\n",
    "    bandelnr_match = re.match(r'^(\\d+(?:/\\d+)*)', bandel)\n",
    "    if bandelnr_match:\n",
    "        bandelnr = bandelnr_match.group(0).replace('/', ', ')\n",
    "        bandelnamn = bandel[len(bandelnr_match.group(0)):].strip()\n",
    "    else:\n",
    "        bandelnr = ''\n",
    "        bandelnamn = bandel.strip()\n",
    "    return pd.Series([bandelnr, bandelnamn])\n",
    "\n",
    "# Apply the parsing function to create two new columns\n",
    "servicekontrakt_df[['Bandelnr', 'Bandelnamn']] = servicekontrakt_df['Bandel'].apply(parse_bandel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the column Tidsperiod (20XX - 20YY) into two new columns 'Start_year' and 'End_year'\n",
    "def parse_tidsperiod(tidsperiod):\n",
    "    import re\n",
    "    # first remove spaces in tidsperiod\n",
    "    tidsperiod = tidsperiod.replace(' ', '')\n",
    "    tidsperiod_match = re.match(r'^(\\d{4})-(\\d{4})$', tidsperiod)\n",
    "    if tidsperiod_match:\n",
    "        start_year = int(tidsperiod_match.group(1))\n",
    "        end_year = int(tidsperiod_match.group(2))\n",
    "    else:\n",
    "        start_year = None\n",
    "        end_year = None\n",
    "    return pd.Series([start_year, end_year])\n",
    "\n",
    "# Apply the parsing function to create two new columns\n",
    "servicekontrakt_df[['Start_year', 'End_year']] = servicekontrakt_df['Tidsperiod'].apply(parse_tidsperiod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where start_year is 2024 or later\n",
    "servicekontrakt_df_T23 = servicekontrakt_df[servicekontrakt_df['Start_year'] < 2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add the corresponding distances (lengths) for the identified bandels using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Global cache for lengths and the graph\n",
    "langd_cache = {}\n",
    "GLOBAL_GRAPH = None\n",
    "\n",
    "# Step 1: Create a mapping from Plats_sign (full name) to Banlangd\n",
    "station_length_lookup = dictionary_df.set_index('Plats_sign')['Banlangd'].to_dict()\n",
    "\n",
    "### Utility Functions ###\n",
    "\n",
    "def initialize_global_graph(dictionary_df):\n",
    "    \"\"\"Initialize the global graph once\"\"\"\n",
    "    global GLOBAL_GRAPH\n",
    "    if GLOBAL_GRAPH is None:\n",
    "        bdl_df = dictionary_df[(dictionary_df['BdlNr'] >= 2) & (dictionary_df['BdlNr'] <= 990)]\n",
    "        GLOBAL_GRAPH = nx.Graph()  # Undirected graph to simulate bidirectional connections\n",
    "        for _, row in bdl_df.iterrows():\n",
    "            if pd.notna(row['Forbind']):\n",
    "                start, end = row['Forbind'].split('-')\n",
    "                length = row['Banlangd']\n",
    "                GLOBAL_GRAPH.add_edge(start.strip(), end.strip(), length=length)\n",
    "\n",
    "def calculate_sum_langd(forbind_list, dictionary_df):\n",
    "    if not forbind_list or forbind_list == '':\n",
    "        return None\n",
    "    \n",
    "    # Check cache\n",
    "    cache_key = (forbind_list)\n",
    "    if cache_key in langd_cache:\n",
    "        return langd_cache[cache_key]\n",
    "    \n",
    "    # Initialize global graph if not already done\n",
    "    if GLOBAL_GRAPH is None:\n",
    "        initialize_global_graph(dictionary_df)\n",
    "    \n",
    "    # Split and clean the forbind_list\n",
    "    forbinds = [f.strip() for f in forbind_list.split(',')]\n",
    "    stations = [station for forbind in forbinds for station in forbind.split('-')]\n",
    "    first_station = stations[0]\n",
    "    last_station = stations[-1]\n",
    "    \n",
    "    # Check if first and last stations are enclosed in parentheses\n",
    "    include_first_station = not (first_station.startswith('(') and first_station.endswith(')'))\n",
    "    include_last_station = not (last_station.startswith('(') and last_station.endswith(')'))\n",
    "    \n",
    "    # Remove parentheses for lookup in the graph\n",
    "    first_station_cleaned = first_station.strip('()')\n",
    "    last_station_cleaned = last_station.strip('()')\n",
    "    \n",
    "    # Check if stations exist in graph\n",
    "    if first_station_cleaned not in GLOBAL_GRAPH or last_station_cleaned not in GLOBAL_GRAPH:\n",
    "        langd_cache[cache_key] = None\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path_length = nx.shortest_path_length(\n",
    "            GLOBAL_GRAPH, \n",
    "            source=first_station_cleaned, \n",
    "            target=last_station_cleaned, \n",
    "            weight='length'\n",
    "        )\n",
    "        \n",
    "        # Calculate length of intermediate stations\n",
    "        shortest_path_stations = nx.shortest_path(\n",
    "            GLOBAL_GRAPH, \n",
    "            source=first_station_cleaned, \n",
    "            target=last_station_cleaned\n",
    "        )\n",
    "        intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "        station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "        \n",
    "        # Add lengths of first and last stations based on inclusion rules\n",
    "        if include_first_station:\n",
    "            station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "        if include_last_station:\n",
    "            station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "        total_length = path_length + station_length_sum\n",
    "        langd_cache[cache_key] = total_length\n",
    "        return total_length\n",
    "        \n",
    "    except nx.NetworkXNoPath:\n",
    "        langd_cache[cache_key] = None\n",
    "        return None  # No path found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "\n",
    "# # Global cache for lengths\n",
    "# langd_cache = {}\n",
    "\n",
    "# # Step 1: Create a mapping from Plats_sign (full name) to Banlangd\n",
    "# station_length_lookup = dictionary_df.set_index('Plats_sign')['Banlangd'].to_dict()\n",
    "\n",
    "# ### Utility Functions ###\n",
    "\n",
    "# # Function to build a bidirectional graph from the DataFrame\n",
    "# def build_bidirectional_graph(dictionary_df, bdl_range):\n",
    "#     # Filter for the given BdlNr range\n",
    "#     bdl_df = dictionary_df[(dictionary_df['BdlNr'] >= bdl_range[0]) & (dictionary_df['BdlNr'] <= bdl_range[1])]\n",
    "    \n",
    "#     G = nx.Graph()  # Undirected graph to simulate bidirectional connections\n",
    "#     for _, row in bdl_df.iterrows():\n",
    "#         if pd.notna(row['Forbind']):\n",
    "#             start, end = row['Forbind'].split('-')\n",
    "#             length = row['Banlangd']\n",
    "#             G.add_edge(start.strip(), end.strip(), length=length)  # Add bidirectional edges\n",
    "#     return G\n",
    "\n",
    "\n",
    "# def calculate_sum_langd(forbind_list, identified_bdlnr, dictionary_df):\n",
    "#     if not forbind_list or forbind_list == '':\n",
    "#         return None\n",
    "    \n",
    "#     # Check cache\n",
    "#     cache_key = (forbind_list, identified_bdlnr)\n",
    "#     if cache_key in langd_cache:\n",
    "#         return langd_cache[cache_key]\n",
    "    \n",
    "#     # Split and clean the forbind_list\n",
    "#     forbinds = [f.strip() for f in forbind_list.split(',')]\n",
    "#     stations = [station for forbind in forbinds for station in forbind.split('-')]\n",
    "#     first_station = stations[0]\n",
    "#     last_station = stations[-1]\n",
    "    \n",
    "#     # Check if first and last stations are enclosed in parentheses\n",
    "#     include_first_station = not (first_station.startswith('(') and first_station.endswith(')'))\n",
    "#     include_last_station = not (last_station.startswith('(') and last_station.endswith(')'))\n",
    "    \n",
    "#     # Remove parentheses for lookup in the graph\n",
    "#     first_station_cleaned = first_station.strip('()')\n",
    "#     last_station_cleaned = last_station.strip('()')\n",
    "    \n",
    "#     # Build the smaller graph first\n",
    "#     small_range = (identified_bdlnr, identified_bdlnr)\n",
    "#     graph = build_bidirectional_graph(dictionary_df, small_range)\n",
    "    \n",
    "#     # Try pathfinding in the small graph\n",
    "#     try:\n",
    "#         if first_station_cleaned in graph and last_station_cleaned in graph:\n",
    "#             path_length = nx.shortest_path_length(\n",
    "#                 graph, source=first_station_cleaned, target=last_station_cleaned, weight='length'\n",
    "#             )\n",
    "            \n",
    "#             # Calculate length of intermediate stations\n",
    "#             shortest_path_stations = nx.shortest_path(graph, source=first_station_cleaned, target=last_station_cleaned)\n",
    "#             intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "#             station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "\n",
    "#             # Add lengths of first and last stations based on inclusion rules\n",
    "#             if include_first_station:\n",
    "#                 station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "#             if include_last_station:\n",
    "#                 station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "#             total_length = path_length + station_length_sum\n",
    "#             langd_cache[cache_key] = total_length\n",
    "#             return total_length\n",
    "#     except nx.NetworkXNoPath:\n",
    "#         pass  # Path not found, fall back to larger graph\n",
    "    \n",
    "#     # Build the larger graph if needed\n",
    "#     large_range = (2,990)#(max(1, identified_bdlnr - 900), min(990, identified_bdlnr + 900))\n",
    "#     graph = build_bidirectional_graph(dictionary_df, large_range)\n",
    "    \n",
    "#     # Check again in the larger graph\n",
    "#     if first_station_cleaned not in graph or last_station_cleaned not in graph:\n",
    "#         langd_cache[cache_key] = None\n",
    "#         return None\n",
    "    \n",
    "#     try:\n",
    "#         path_length = nx.shortest_path_length(\n",
    "#             graph, source=first_station_cleaned, target=last_station_cleaned, weight='length'\n",
    "#         )\n",
    "        \n",
    "#         # Calculate length of intermediate stations\n",
    "#         shortest_path_stations = nx.shortest_path(graph, source=first_station_cleaned, target=last_station_cleaned)\n",
    "#         intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "#         station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "        \n",
    "#         # Add lengths of first and last stations based on inclusion rules\n",
    "#         if include_first_station:\n",
    "#             station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "#         if include_last_station:\n",
    "#             station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "#         total_length = path_length + station_length_sum\n",
    "#         langd_cache[cache_key] = total_length\n",
    "#         return total_length\n",
    "#     except nx.NetworkXNoPath:\n",
    "#         langd_cache[cache_key] = None\n",
    "#         return None  # No path found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from Plats (full name) to Plats_sign (short code)\n",
    "name_to_code_mapping = dictionary_df.set_index('Plats_sign')['Plats'].to_dict()\n",
    "\n",
    "def convert_bandelnamn_to_codes(bandelnamn):\n",
    "    # Split by dash and preserve parentheses\n",
    "    stations = bandelnamn.split('-')\n",
    "    \n",
    "    # Create a case-insensitive mapping of full names to codes\n",
    "    name_to_code_mapping_lower = {\n",
    "        str(v).lower(): str(k) for k, v in name_to_code_mapping.items()\n",
    "    }\n",
    "    \n",
    "    # Detailed conversion with original names preserved\n",
    "    station_details = []\n",
    "    station_codes = []\n",
    "\n",
    "    for name in stations:\n",
    "        stripped_name = str(name).strip()\n",
    "        has_parentheses = stripped_name.startswith('(') and stripped_name.endswith(')')\n",
    "        \n",
    "        # Remove parentheses temporarily for lookup\n",
    "        name_without_parentheses = stripped_name[1:-1] if has_parentheses else stripped_name\n",
    "        \n",
    "        # Try case-insensitive matching with original name\n",
    "        code = name_to_code_mapping_lower.get(name_without_parentheses.lower())\n",
    "        \n",
    "        # If no code found, try appending \" central\"\n",
    "        central_name = \"\"\n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses} central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses}s central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "        station_details.append({\n",
    "            'original_name': stripped_name,\n",
    "            'tried_name': central_name if code and code != name_without_parentheses else None,\n",
    "            'station_code': code\n",
    "        })\n",
    "        \n",
    "        if code:\n",
    "            # Add parentheses back if they were present\n",
    "            formatted_code = f\"({code})\" if has_parentheses else code\n",
    "            station_codes.append(formatted_code)\n",
    "    \n",
    "    return {\n",
    "        'station_details': station_details,\n",
    "        'station_codes': station_codes,\n",
    "        'short_path': '-'.join(station_codes) if station_codes else None\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 2: Prepare the dataframe with detailed conversion results\n",
    "def prepare_bandelnamn_conversion(servicekontrakt_df):\n",
    "    # Apply the conversion function\n",
    "    conversion_results = servicekontrakt_df['Bandelnamn'].apply(convert_bandelnamn_to_codes)\n",
    "    \n",
    "    # Extract details into separate columns\n",
    "    servicekontrakt_df = servicekontrakt_df.copy()\n",
    "    servicekontrakt_df['station_details'] = conversion_results.apply(lambda x: x['station_details'])\n",
    "    servicekontrakt_df['original_station_names'] = servicekontrakt_df['station_details'].apply(\n",
    "        lambda x: [detail['original_name'] for detail in x]\n",
    "    )\n",
    "    servicekontrakt_df['station_codes'] = conversion_results.apply(lambda x: x['station_codes'])\n",
    "    servicekontrakt_df['short_path'] = conversion_results.apply(lambda x: x['short_path'])\n",
    "    \n",
    "    return servicekontrakt_df\n",
    "\n",
    "\n",
    "def calculate_sum_langd_for_bandelnamn(row, dictionary_df):\n",
    "\n",
    "    row_bandel = None\n",
    "    row_forbind = None\n",
    "    if 'identified_BdlNr' in row and pd.notna(row['identified_BdlNr']): # for TCRs\n",
    "        row_bandel = int(row['identified_BdlNr'])\n",
    "        row_forbind = row['förbind_list']\n",
    "    else: # for contracts\n",
    "        row_bandel = int(row['Bandelnr'])\n",
    "        row_forbind = row['short_path']\n",
    "\n",
    "    # If short_path is None, return None\n",
    "    if not row_forbind:\n",
    "        return None\n",
    "\n",
    "    # Case 1: Single station\n",
    "    if '-' not in row_forbind:\n",
    "        single_station = row_forbind\n",
    "        \n",
    "        # Find the row in dictionary_df where Plats_sign matches the single station\n",
    "        matching_station = dictionary_df[dictionary_df['Plats_sign'] == single_station]\n",
    "        \n",
    "    \n",
    "        # keep only rows where BdlNr is same as row['Bandelnr']\n",
    "        matching_station = matching_station[matching_station['BdlNr'] == row_bandel]\n",
    "\n",
    "        # If no matching station is found, return None\n",
    "        if matching_station.empty:\n",
    "            return None\n",
    "        \n",
    "        # Get the station's length as the sum of all values in matching_station['Banlangd']\n",
    "        # station_length = matching_station['Banlangd'].iloc[0]\n",
    "        station_length = matching_station['Banlangd'].sum()\n",
    "        \n",
    "        return station_length\n",
    "        # # Find neighboring connections in the Forbind column (skip NaN values)\n",
    "        # neighbors = dictionary_df[\n",
    "        #     dictionary_df['Forbind'].notna() &  # Skip NaN values\n",
    "        #     dictionary_df['Forbind'].str.contains(f\"^{single_station}-|-{single_station}$\", regex=True)\n",
    "        # ]\n",
    "        \n",
    "        # # Calculate half the lengths of the neighboring connections\n",
    "        # half_neighbor_lengths = 0\n",
    "        # for _, neighbor in neighbors.iterrows():\n",
    "        #     length = neighbor['Banlangd']\n",
    "        #     half_neighbor_lengths += length / 2\n",
    "        \n",
    "        # # Return the total length: station's length + half the neighbors' lengths\n",
    "        # return station_length + half_neighbor_lengths\n",
    "\n",
    "    # Case 2: Multiple stations (existing logic)\n",
    "    return calculate_sum_langd(row_forbind, dictionary_df)\n",
    "\n",
    "\n",
    "# Apply the steps\n",
    "# 1. First, prepare the conversion\n",
    "servicekontrakt_df_T23 = prepare_bandelnamn_conversion(servicekontrakt_df_T23)\n",
    "\n",
    "# remove rows from dictionary_df where BdlNr is 1\n",
    "#dictionary_df = dictionary_df[dictionary_df['BdlNr'] != 1]\n",
    "\n",
    "# 2. Then calculate sum_langd\n",
    "servicekontrakt_df_T23['sum_langd'] = servicekontrakt_df_T23.apply(\n",
    "    lambda row: calculate_sum_langd_for_bandelnamn(row, dictionary_df),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can additionally identify the name of the contract area using bandel and mapping we have from BIS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add 'kontrakt_från_bandel' column by mapping 'Bandel' to the cleaned dictionary\n",
    "def map_bandel_to_contract(bandel, mapping_dict):\n",
    "    # Extract the first Bandelnr if Bandelnr contains multiple values (e.g., \"451, 452\")\n",
    "    first_bandel = int(bandel)\n",
    "    return mapping_dict.get(first_bandel, None)\n",
    "\n",
    "servicekontrakt_df_T23['kontrakt_från_bandel'] = servicekontrakt_df_T23['Bandelnr'].apply(\n",
    "    lambda bandel: map_bandel_to_contract(bandel, bandel_contract_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching TCR:s Förbindelser with bandelar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the column Från trafikplats, we can identify the corresponding bandel using dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file containing matched TCRs for 2024\n",
    "csv_file_path = \"TCR_T23_matched.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "tcr_df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a mapping from 'Plats_sign' to 'BdlNr' for fast lookups\n",
    "trafikplats_to_bandel_map = dictionary_df.set_index('Plats_sign')['BdlNr'].to_dict()\n",
    "\n",
    "#  Create a mapping from 'Forbind' to 'BdlNr' for quick lookups\n",
    "forbind_to_bdl_map = dictionary_df.set_index('Forbind')['BdlNr'].to_dict()\n",
    "\n",
    "# replace in column Från trafikplats all values of Rus in tcr_df (not in dictionary) with Jho\n",
    "tcr_df['Från trafikplats'] = tcr_df['Från trafikplats'].replace('Rus', 'Jho')\n",
    "tcr_df['Från trafikplats'] = tcr_df['Från trafikplats'].replace('Ksc', 'Ks')\n",
    "tcr_df['Från trafikplats'] = tcr_df['Från trafikplats'].replace('Sta', 'Äs')\n",
    "tcr_df['Från trafikplats'] = tcr_df['Från trafikplats'].replace('Les', 'Alh')\n",
    "# Tul - Söd cannot be connected because no forbind at Gau\n",
    "\n",
    "# Step 2: Use .map() for vectorized lookup\n",
    "tcr_df['identified_BdlNr'] = tcr_df['Från trafikplats'].map(trafikplats_to_bandel_map)\n",
    "\n",
    "\n",
    "def fill_missing_bandel(df):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # For each TCR-id group\n",
    "    for tcr_id in df['TCR-id'].unique():\n",
    "        # Get rows for this TCR-id\n",
    "        mask = df['TCR-id'] == tcr_id\n",
    "        tcr_rows = df[mask]\n",
    "        \n",
    "        # Find rows with missing bandel\n",
    "        missing_mask = tcr_rows['identified_BdlNr'].isna()\n",
    "        missing_indices = tcr_rows[missing_mask].index\n",
    "        \n",
    "        # For each missing value\n",
    "        for idx in missing_indices:\n",
    "            current_seq = df.loc[idx, 'Platssekvensnummer']\n",
    "            \n",
    "            # Look for neighboring rows (same TCR-id, sequence number ±1)\n",
    "            neighbors = tcr_rows[\n",
    "                (tcr_rows['Platssekvensnummer'].isin([current_seq - 1, current_seq + 1]))\n",
    "            ]\n",
    "            \n",
    "            # If we found any valid neighbors, use their bandel number\n",
    "            valid_neighbors = neighbors[neighbors['identified_BdlNr'].notna()]\n",
    "            if not valid_neighbors.empty:\n",
    "                df.loc[idx, 'identified_BdlNr'] = valid_neighbors['identified_BdlNr'].iloc[0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to fill missing bandel numbers\n",
    "tcr_df = fill_missing_bandel(tcr_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some rows where the bandel is not identified because the Från trafikplats is not in the dictionary. For these rows we will use the bandel that is identified in related row, i.e., rows with the same TCR-id and with Platssekvensnummer which is neighboring (i.e., Platssekvensnummer = Platssekvensnummer of the unidentified bandel row minus or plus 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbdouAA\\AppData\\Local\\Temp\\ipykernel_20952\\1765125021.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_tcr_df = tcr_df.groupby(['TCR-id', 'Starttid'], as_index=False).apply(get_first_last_rows)\n"
     ]
    }
   ],
   "source": [
    "# Group by TCR-id and Starttid\n",
    "def get_first_last_rows(group):\n",
    "    # Get min and max Platssekvensnummer rows\n",
    "    first_row = group[group['Platssekvensnummer'] == group['Platssekvensnummer'].min()]\n",
    "    last_row = group[group['Platssekvensnummer'] == group['Platssekvensnummer'].max()]\n",
    "    return pd.concat([first_row, last_row])\n",
    "\n",
    "# Apply the function to each group\n",
    "filtered_tcr_df = tcr_df.groupby(['TCR-id', 'Starttid'], as_index=False).apply(get_first_last_rows)\n",
    "\n",
    "# Reset index if needed\n",
    "filtered_tcr_df = filtered_tcr_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calculating the length between two consecutive places/rows, we need to reformat the tcr_df so that in each row we combine the row with the next row (in Platssekvensnummer), if any (until final row in the sequence).  We create a new column called förbind_list which will contactenate Från trafikplats of two consecutive rows, e.g., A-B (where A is trafikplats of the first row and B is the second), next row will have B-C, etc. until the final förbind in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sort the DataFrame by 'TCR-id' and 'Platssekvensnummer'\n",
    "filtered_tcr_df = filtered_tcr_df.sort_values(by=['TCR-id', 'Starttid', 'Platssekvensnummer']).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Create 'next_trafikplats' and 'next_Från_inkluderad'\n",
    "filtered_tcr_df['next_trafikplats'] = filtered_tcr_df.groupby(['TCR-id', 'Starttid'])['Från trafikplats'].shift(-1)\n",
    "filtered_tcr_df['next_Från_inkluderad'] = filtered_tcr_df.groupby(['TCR-id', 'Starttid'])['Från inkluderad'].shift(-1)\n",
    "\n",
    "# Step 5: Create 'förbind_list' with conditional parentheses\n",
    "def format_trafikplats(trafikplats, inkluderad):\n",
    "    \"\"\"Format trafikplats name with parentheses based on inclusion status.\"\"\"\n",
    "    if inkluderad != 'Helt':\n",
    "        return f\"({trafikplats})\"\n",
    "    return trafikplats\n",
    "\n",
    "def create_förbind(row):\n",
    "    \"\"\"Create förbind string for a row, connecting two trafikplats names.\"\"\"\n",
    "    if pd.isna(row['next_trafikplats']):\n",
    "        return None\n",
    "\n",
    "    if(row['Från trafikplats'] == row['next_trafikplats']):\n",
    "        return f\"{row['Från trafikplats']}\"\n",
    "\n",
    "    from_tp = format_trafikplats(row['Från trafikplats'], row['Från inkluderad'])\n",
    "    to_tp = format_trafikplats(row['next_trafikplats'], row['next_Från_inkluderad'])\n",
    "\n",
    "\n",
    "    return f\"{from_tp}-{to_tp}\"\n",
    "\n",
    "# Apply the function to create förbind_list\n",
    "filtered_tcr_df['förbind_list'] = filtered_tcr_df.apply(create_förbind, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Remove temporary 'next_trafikplats' and 'next_Från_inkluderad' columns\n",
    "#filtered_tcr_df = filtered_tcr_df.drop(columns=['next_trafikplats', 'next_Från_inkluderad'])\n",
    "\n",
    "# Step 7: Remove the final row in each sequence\n",
    "filtered_tcr_df = filtered_tcr_df.dropna(subset=['förbind_list']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy\n",
    "tcr_df = filtered_tcr_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the identified_BdlNr given the förbind_list. So, if the förbind_list is in the dictionary (column Forbind), and the corresponding BdlNr is different then the current identified_BdlNr, then update it. Otherwise leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Create a mapping from 'Forbind' to 'BdlNr' for quick lookups\n",
    "# forbind_to_bdl_map = dictionary_df.set_index('Forbind')['BdlNr'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Update 'identified_BdlNr' based on 'förbind_list'\n",
    "# def update_bandel(row):\n",
    "#     # Check if 'förbind_list' exists in the dictionary\n",
    "#     if row['förbind_list'] in forbind_to_bdl_map:\n",
    "#         new_bdl_nr = forbind_to_bdl_map[row['förbind_list']]\n",
    "#         # Update only if the new BdlNr is different\n",
    "#         if new_bdl_nr != row['identified_BdlNr']:\n",
    "#             return new_bdl_nr\n",
    "    \n",
    "#     # If not found, try the inverted link\n",
    "#     inverted_link = '-'.join(reversed(row['förbind_list'].split('-')))\n",
    "#     if inverted_link in forbind_to_bdl_map:\n",
    "#         new_bdl_nr = forbind_to_bdl_map[inverted_link]\n",
    "#         # Update only if the new BdlNr is different\n",
    "#         if new_bdl_nr != row['identified_BdlNr']:\n",
    "#             return new_bdl_nr\n",
    "    \n",
    "#     # If no update is needed or not found, return the current value\n",
    "#     return row['identified_BdlNr']\n",
    "\n",
    "# # Apply the function to update 'identified_BdlNr'\n",
    "# tcr_df['identified_BdlNr'] = tcr_df.apply(update_bandel, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have identified BdlNR, we can use servicekontrakt_df to add a column with Kontraktsområdesnamn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the contract map with float conversion\n",
    "contract_map = servicekontrakt_df_T23.drop_duplicates(subset=['Bandelnr']).copy()\n",
    "contract_map['Bandelnr'] = contract_map['Bandelnr'].astype(float)\n",
    "contract_map = contract_map.set_index('Bandelnr')['Kontraktsområdesnamn'].to_dict()\n",
    "\n",
    "# Map with the contract map\n",
    "tcr_df['identified_BdlNr'] = tcr_df['identified_BdlNr'].astype(float)\n",
    "tcr_df['Kontraktsområdesnamn'] = tcr_df['identified_BdlNr'].map(contract_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also include a similar column with contract name, this one is based on BIS file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional mapping using the bandel_contract_dict\n",
    "tcr_df['kontrakt_från_bandel'] = tcr_df['identified_BdlNr'].map(bandel_contract_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No that we have identified BdlNr, we want to get the total length langd (which is in meter) of the Från trafikplats  and put it in a column (sum_langd). The idea is to use the order of forbind_list and look for the corresponding rows in dictionary_df (within same bandelnr = identified BdlNr) and accumulate the lenght in column dictionary_df(Banlangd). The forbind_list are normally linked, e.g., A-B, B-C, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create the 'sum_langd' column for tcr_df\n",
    "# tcr_df['sum_langd'] = tcr_df.apply(\n",
    "#     lambda row: calculate_sum_langd(\n",
    "#         row['förbind_list'], \n",
    "#         row['identified_BdlNr'], \n",
    "#         dictionary_df\n",
    "#     ), \n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "tcr_df['sum_langd'] = tcr_df.apply(\n",
    "    lambda row: calculate_sum_langd_for_bandelnamn(row, dictionary_df),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Kontraktsområdesnamn, Tidsperiod, Bandel, TPA timmar per år, TPA dagar per år, TPA veckor per år, TPA timmar natt per år, TPA timmar helg per år, EJ TPA timmar per år, EJ TPA dagar per år, EJ TPA veckor per år, EJ TPA timmar natt per år, EJ TPA timmar helg per år, Bandelnr, Bandelnamn, Start_year, End_year, station_details, original_station_names, station_codes, short_path, sum_langd, kontrakt_från_bandel, original_station_names_len, station_codes_len]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# check that for each row of servicekontrakt_df_T23 the number of item in the list original_station_names is the same as in station_codes\n",
    "servicekontrakt_df_T23['original_station_names_len'] = servicekontrakt_df_T23['original_station_names'].apply(lambda x: len(x))\n",
    "servicekontrakt_df_T23['station_codes_len'] = servicekontrakt_df_T23['station_codes'].apply(lambda x: len(x))\n",
    "# print rows where these are different\n",
    "print(servicekontrakt_df_T23[servicekontrakt_df_T23['original_station_names_len'] != servicekontrakt_df_T23['station_codes_len']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TCR-id          Område Klassificering             Starttid  \\\n",
      "559      214            Väst      Medelstor  2023-04-06 02:00:00   \n",
      "560      218            Väst          Liten  2023-09-10 22:15:00   \n",
      "561      218            Väst          Liten  2023-09-11 22:15:00   \n",
      "562      218            Väst          Liten  2023-09-12 22:15:00   \n",
      "563      218            Väst          Liten  2023-09-13 22:15:00   \n",
      "...      ...             ...            ...                  ...   \n",
      "6908     688  Nord/Mellersta          Liten  2023-08-30 03:35:00   \n",
      "6909     688  Nord/Mellersta          Liten  2023-08-31 03:35:00   \n",
      "6910     688  Nord/Mellersta          Liten  2023-09-01 03:35:00   \n",
      "6911     688  Nord/Mellersta          Liten  2023-09-02 03:35:00   \n",
      "6912     688  Nord/Mellersta          Liten  2023-09-03 03:35:00   \n",
      "\n",
      "                  Sluttid Från trafikplats Från linjespår Från inkluderad  \\\n",
      "559   2023-04-10 02:00:00             Kogr              E            Helt   \n",
      "560   2023-09-11 05:15:00             Kogr              E            Helt   \n",
      "561   2023-09-12 05:15:00             Kogr              E            Helt   \n",
      "562   2023-09-13 05:15:00             Kogr              E            Helt   \n",
      "563   2023-09-14 05:15:00             Kogr              E            Helt   \n",
      "...                   ...              ...            ...             ...   \n",
      "6908  2023-08-30 09:40:00             Bjgr              E            Helt   \n",
      "6909  2023-08-31 09:40:00             Bjgr              E            Helt   \n",
      "6910  2023-09-01 09:40:00             Bjgr              E            Helt   \n",
      "6911  2023-09-02 09:40:00             Bjgr              E            Helt   \n",
      "6912  2023-09-03 09:40:00             Bjgr              E            Helt   \n",
      "\n",
      "      Platssekvensnummer                                        Beskrivning  \\\n",
      "559                    1                           Servicefönster underhåll   \n",
      "560                    1  Servicefönster underhåll, ny timmerterminal Bä...   \n",
      "561                    1  Servicefönster underhåll, ny timmerterminal Bä...   \n",
      "562                    1  Servicefönster underhåll, ny timmerterminal Bä...   \n",
      "563                    1  Servicefönster underhåll, ny timmerterminal Bä...   \n",
      "...                  ...                                                ...   \n",
      "6908                   1  Servicefönster Underhåll Riksgränsen-(Peuravaa...   \n",
      "6909                   1  Servicefönster Underhåll Riksgränsen-(Peuravaa...   \n",
      "6910                   1  Servicefönster Underhåll Riksgränsen-(Peuravaa...   \n",
      "6911                   1  Servicefönster Underhåll Riksgränsen-(Peuravaa...   \n",
      "6912                   1  Servicefönster Underhåll Riksgränsen-(Peuravaa...   \n",
      "\n",
      "      ... tid_timmar Relaterad åtgärdsnummer  Relaterad kontrakt  \\\n",
      "559   ...  96.000000                147910.0                 NaN   \n",
      "560   ...   7.000000                147910.0                 NaN   \n",
      "561   ...   7.000000                147910.0                 NaN   \n",
      "562   ...   7.000000                147910.0                 NaN   \n",
      "563   ...   7.000000                147910.0                 NaN   \n",
      "...   ...        ...                     ...                 ...   \n",
      "6908  ...   6.083333                170875.0                 NaN   \n",
      "6909  ...   6.083333                170875.0                 NaN   \n",
      "6910  ...   6.083333                170875.0                 NaN   \n",
      "6911  ...   6.083333                170875.0                 NaN   \n",
      "6912  ...   6.083333                170875.0                 NaN   \n",
      "\n",
      "      identified_BdlNr  next_trafikplats  next_Från_inkluderad förbind_list  \\\n",
      "559              636.0              Skbl                   NaN  Kogr-(Skbl)   \n",
      "560              636.0              Skbl                   NaN  Kogr-(Skbl)   \n",
      "561              636.0              Skbl                   NaN  Kogr-(Skbl)   \n",
      "562              636.0              Skbl                   NaN  Kogr-(Skbl)   \n",
      "563              636.0              Skbl                   NaN  Kogr-(Skbl)   \n",
      "...                ...               ...                   ...          ...   \n",
      "6908             111.0               Pea                   NaN   Bjgr-(Pea)   \n",
      "6909             111.0               Pea                   NaN   Bjgr-(Pea)   \n",
      "6910             111.0               Pea                   NaN   Bjgr-(Pea)   \n",
      "6911             111.0               Pea                   NaN   Bjgr-(Pea)   \n",
      "6912             111.0               Pea                   NaN   Bjgr-(Pea)   \n",
      "\n",
      "     Kontraktsområdesnamn       kontrakt_från_bandel sum_langd  \n",
      "559                   NaN  Värmland/Dalslandsbanorna       NaN  \n",
      "560                   NaN  Värmland/Dalslandsbanorna       NaN  \n",
      "561                   NaN  Värmland/Dalslandsbanorna       NaN  \n",
      "562                   NaN  Värmland/Dalslandsbanorna       NaN  \n",
      "563                   NaN  Värmland/Dalslandsbanorna       NaN  \n",
      "...                   ...                        ...       ...  \n",
      "6908      Norra Malmbanan            Norra Malmbanan       NaN  \n",
      "6909      Norra Malmbanan            Norra Malmbanan       NaN  \n",
      "6910      Norra Malmbanan            Norra Malmbanan       NaN  \n",
      "6911      Norra Malmbanan            Norra Malmbanan       NaN  \n",
      "6912      Norra Malmbanan            Norra Malmbanan       NaN  \n",
      "\n",
      "[240 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# check if there are any rows in tcr_df where sum_langd is None, print them\n",
    "print(tcr_df[tcr_df['sum_langd'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find if all bandel numbers in tcr_df and servicekontrakt_df_T23 belong to the same contract, print the bandel numbers that do not belong to the same contract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column Total timmar per år by adding TPA timmar per år and EJ TPA timmar per år\n",
    "servicekontrakt_df_T23['Total timmar per år'] = servicekontrakt_df_T23['TPA timmar per år'] + servicekontrakt_df_T23['EJ TPA timmar per år']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for excel\n",
    "excel_file_path = \"Servicekontrakt_per_bandel_matched_T23.xlsx\"\n",
    "servicekontrakt_df_to_export = servicekontrakt_df_T23[['Kontraktsområdesnamn', 'kontrakt_från_bandel', 'Tidsperiod', 'Bandel', 'TPA timmar per år',\n",
    "       'TPA dagar per år', 'TPA veckor per år', 'TPA timmar natt per år',\n",
    "       'TPA timmar helg per år', 'EJ TPA timmar per år', 'EJ TPA dagar per år',\n",
    "       'EJ TPA veckor per år', 'EJ TPA timmar natt per år',\n",
    "       'EJ TPA timmar helg per år', 'Total timmar per år', 'Bandelnr',\n",
    "       'Bandelnamn', 'sum_langd']]\n",
    "servicekontrakt_df_to_export.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Keep only the specified columns in tcr_df\n",
    "tcr_df_to_export = tcr_df[['TCR-id', 'Klassificering',\n",
    "                 'Starttid', 'Sluttid', 'Servicefönster_nya_kategorier',\n",
    "                 'Relaterade TPÅ:er', 'tid_timmar', 'Relaterad åtgärdsnummer',\n",
    "                 'förbind_list', 'identified_BdlNr', 'sum_langd', 'Kontraktsområdesnamn', 'kontrakt_från_bandel']]\n",
    "\n",
    "# Step 10: Export the DataFrame to an Excel file\n",
    "excel_file_path = \"TCR_T23_matched_bandelar.xlsx\"\n",
    "tcr_df_to_export.to_excel(excel_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
