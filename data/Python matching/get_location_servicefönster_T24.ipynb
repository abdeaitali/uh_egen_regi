{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching the location of each TCR to contracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by reading the cleaned and matched TCRs for 2024 which are matched with contracts number. As well as Karins matching between bandel and number of hours of servicefönster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCR:s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file containing matched TCRs for 2024\n",
    "csv_file_path = \"TCR_T24_matched.csv\"\n",
    "\n",
    "\n",
    "# Option 1: Specify the delimiter (try ',' or ';')\n",
    "try:\n",
    "    tcr_df = pd.read_csv(csv_file_path, delimiter=';', on_bad_lines='skip', encoding='utf-8')\n",
    "except pd.errors.ParserError:\n",
    "    tcr_df = pd.read_csv(csv_file_path, delimiter=',', on_bad_lines='skip', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Förbindelser och BIS (bandelar - kontrakt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Excel file containing the dictionary for bandel matching, and keep only relevant columns and bandel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_file_path = \"Förbindelselinje_2024.xlsx\"\n",
    "\n",
    "# Read the entire dictionary into a DataFrame\n",
    "dictionary_df = pd.read_excel(dictionary_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and sheet details\n",
    "excel_file_path = \"BIS_24_kontrakt_bandel_plats.xlsx\"\n",
    "sheet_name = \"BIS 2024-01-09\"\n",
    "\n",
    "# Load the Excel file\n",
    "bis_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Step 1: Remove duplicates from the mapping\n",
    "bandel_to_contract_map = bis_df[['Bandel_nummer', 'UH_kontraktsområde']].drop_duplicates()\n",
    "\n",
    "# Step 2: Filter out rows where UH_kontraktsområde is NaN or 'Ingår inte i något kontrakt'\n",
    "bandel_to_contract_map = bandel_to_contract_map[\n",
    "    bandel_to_contract_map['UH_kontraktsområde'].notna() & \n",
    "    (bandel_to_contract_map['UH_kontraktsområde'] != 'Ingår inte i något kontrakt')\n",
    "]\n",
    "\n",
    "# Convert to a dictionary for fast lookups\n",
    "bandel_contract_dict = bandel_to_contract_map.set_index('Bandel_nummer')['UH_kontraktsområde'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Karin's bandelar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the Excel file containing service contracts for each bandel\n",
    "excel_file_path = \"servicekontrakt_per_bandel_Abdou.xlsx\"\n",
    "sheet_name = \"uppdaterad\"\n",
    "\n",
    "# Read the specific sheet 'T24' into a DataFrame\n",
    "servicekontrakt_df = pd.read_excel(excel_file_path, sheet_name=sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only T24 contracts\n",
    "servicekontrakt_df = servicekontrakt_df[(servicekontrakt_df['T24'] == 1)]\n",
    "servicekontrakt_df = servicekontrakt_df.drop(columns=['T24', 'T23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the 'Bandel' column into two new columns 'Bandelnr' and 'Bandelnamn'\n",
    "def parse_bandel(bandel):\n",
    "    import re\n",
    "    bandelnr_match = re.match(r'^(\\d+(?:/\\d+)*)', bandel)\n",
    "    if bandelnr_match:\n",
    "        bandelnr = bandelnr_match.group(0).replace('/', ', ')\n",
    "        bandelnamn = bandel[len(bandelnr_match.group(0)):].strip()\n",
    "    else:\n",
    "        bandelnr = ''\n",
    "        bandelnamn = bandel.strip()\n",
    "    return pd.Series([bandelnr, bandelnamn])\n",
    "\n",
    "# Apply the parsing function to create two new columns\n",
    "servicekontrakt_df[['Bandelnr', 'Bandelnamn']] = servicekontrakt_df['Bandel'].apply(parse_bandel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add the corresponding distances (lengths) for the identified bandels using the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Global cache for lengths\n",
    "langd_cache = {}\n",
    "\n",
    "# Step 1: Create a mapping from Plats_sign (full name) to Banlangd\n",
    "station_length_lookup = dictionary_df.set_index('Plats_sign')['Banlangd'].to_dict()\n",
    "\n",
    "### Utility Functions ###\n",
    "\n",
    "# Function to build a bidirectional graph from the DataFrame\n",
    "def build_bidirectional_graph(dictionary_df, bdl_range):\n",
    "    # Filter for the given BdlNr range\n",
    "    bdl_df = dictionary_df[(dictionary_df['BdlNr'] >= bdl_range[0]) & (dictionary_df['BdlNr'] <= bdl_range[1])]\n",
    "    \n",
    "    G = nx.Graph()  # Undirected graph to simulate bidirectional connections\n",
    "    for _, row in bdl_df.iterrows():\n",
    "        if pd.notna(row['Forbind']):\n",
    "            start, end = row['Forbind'].split('-')\n",
    "            length = row['Banlangd']\n",
    "            G.add_edge(start.strip(), end.strip(), length=length)  # Add bidirectional edges\n",
    "    return G\n",
    "\n",
    "\n",
    "def calculate_sum_langd(forbind_list, identified_bdlnr, dictionary_df):\n",
    "    if not forbind_list or forbind_list == '':\n",
    "        return None\n",
    "    \n",
    "    # Check cache\n",
    "    cache_key = (forbind_list, identified_bdlnr)\n",
    "    if cache_key in langd_cache:\n",
    "        return langd_cache[cache_key]\n",
    "    \n",
    "    # Split and clean the forbind_list\n",
    "    forbinds = [f.strip() for f in forbind_list.split(',')]\n",
    "    stations = [station for forbind in forbinds for station in forbind.split('-')]\n",
    "    first_station = stations[0]\n",
    "    last_station = stations[-1]\n",
    "    \n",
    "    # Check if first and last stations are enclosed in parentheses\n",
    "    include_first_station = not (first_station.startswith('(') and first_station.endswith(')'))\n",
    "    include_last_station = not (last_station.startswith('(') and last_station.endswith(')'))\n",
    "    \n",
    "    # Remove parentheses for lookup in the graph\n",
    "    first_station_cleaned = first_station.strip('()')\n",
    "    last_station_cleaned = last_station.strip('()')\n",
    "    \n",
    "    # Build the smaller graph first\n",
    "    small_range = (identified_bdlnr, identified_bdlnr)\n",
    "    graph = build_bidirectional_graph(dictionary_df, small_range)\n",
    "    \n",
    "    # Try pathfinding in the small graph\n",
    "    try:\n",
    "        if first_station_cleaned in graph and last_station_cleaned in graph:\n",
    "            path_length = nx.shortest_path_length(\n",
    "                graph, source=first_station_cleaned, target=last_station_cleaned, weight='length'\n",
    "            )\n",
    "            \n",
    "            # Calculate length of intermediate stations\n",
    "            shortest_path_stations = nx.shortest_path(graph, source=first_station_cleaned, target=last_station_cleaned)\n",
    "            intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "            station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "\n",
    "            # Add lengths of first and last stations based on inclusion rules\n",
    "            if include_first_station:\n",
    "                station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "            if include_last_station:\n",
    "                station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "            total_length = path_length + station_length_sum\n",
    "            langd_cache[cache_key] = total_length\n",
    "            return total_length\n",
    "    except nx.NetworkXNoPath:\n",
    "        pass  # Path not found, fall back to larger graph\n",
    "    \n",
    "    # Build the larger graph if needed\n",
    "    large_range = (max(1, identified_bdlnr - 30), min(990, identified_bdlnr + 30))\n",
    "    graph = build_bidirectional_graph(dictionary_df, large_range)\n",
    "    \n",
    "    # Check again in the larger graph\n",
    "    if first_station_cleaned not in graph or last_station_cleaned not in graph:\n",
    "        langd_cache[cache_key] = None\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        path_length = nx.shortest_path_length(\n",
    "            graph, source=first_station_cleaned, target=last_station_cleaned, weight='length'\n",
    "        )\n",
    "        \n",
    "        # Calculate length of intermediate stations\n",
    "        shortest_path_stations = nx.shortest_path(graph, source=first_station_cleaned, target=last_station_cleaned)\n",
    "        intermediate_stations = shortest_path_stations[1:-1]  # Exclude first and last station\n",
    "        station_length_sum = sum(station_length_lookup.get(station, 0) for station in intermediate_stations)\n",
    "        \n",
    "        # Add lengths of first and last stations based on inclusion rules\n",
    "        if include_first_station:\n",
    "            station_length_sum += station_length_lookup.get(first_station_cleaned, 0)\n",
    "        if include_last_station:\n",
    "            station_length_sum += station_length_lookup.get(last_station_cleaned, 0)\n",
    "\n",
    "        total_length = path_length + station_length_sum\n",
    "        langd_cache[cache_key] = total_length\n",
    "        return total_length\n",
    "    except nx.NetworkXNoPath:\n",
    "        langd_cache[cache_key] = None\n",
    "        return None  # No path found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from Plats (full name) to Plats_sign (short code)\n",
    "name_to_code_mapping = dictionary_df.set_index('Plats_sign')['Plats'].to_dict()\n",
    "\n",
    "def convert_bandelnamn_to_codes(bandelnamn):\n",
    "    # Split by dash and preserve parentheses\n",
    "    stations = bandelnamn.split('-')\n",
    "    \n",
    "    # Create a case-insensitive mapping of full names to codes\n",
    "    name_to_code_mapping_lower = {\n",
    "        str(v).lower(): str(k) for k, v in name_to_code_mapping.items()\n",
    "    }\n",
    "    \n",
    "    # Detailed conversion with original names preserved\n",
    "    station_details = []\n",
    "    station_codes = []\n",
    "\n",
    "    for name in stations:\n",
    "        stripped_name = str(name).strip()\n",
    "        has_parentheses = stripped_name.startswith('(') and stripped_name.endswith(')')\n",
    "        \n",
    "        # Remove parentheses temporarily for lookup\n",
    "        name_without_parentheses = stripped_name[1:-1] if has_parentheses else stripped_name\n",
    "        \n",
    "        # Try case-insensitive matching with original name\n",
    "        code = name_to_code_mapping_lower.get(name_without_parentheses.lower())\n",
    "        \n",
    "        # If no code found, try appending \" central\"\n",
    "        central_name = \"\"\n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses} central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "        if code is None:\n",
    "            central_name = f\"{name_without_parentheses}s central\"\n",
    "            code = name_to_code_mapping_lower.get(central_name.lower(), None)\n",
    "        \n",
    "        station_details.append({\n",
    "            'original_name': stripped_name,\n",
    "            'tried_name': central_name if code and code != name_without_parentheses else None,\n",
    "            'station_code': code\n",
    "        })\n",
    "        \n",
    "        if code:\n",
    "            # Add parentheses back if they were present\n",
    "            formatted_code = f\"({code})\" if has_parentheses else code\n",
    "            station_codes.append(formatted_code)\n",
    "    \n",
    "    return {\n",
    "        'station_details': station_details,\n",
    "        'station_codes': station_codes,\n",
    "        'short_path': '-'.join(station_codes) if station_codes else None\n",
    "    }\n",
    "\n",
    "\n",
    "# Step 2: Prepare the dataframe with detailed conversion results\n",
    "def prepare_bandelnamn_conversion(servicekontrakt_df):\n",
    "    # Apply the conversion function\n",
    "    conversion_results = servicekontrakt_df['Bandelnamn'].apply(convert_bandelnamn_to_codes)\n",
    "    \n",
    "    # Extract details into separate columns\n",
    "    servicekontrakt_df['station_details'] = conversion_results.apply(lambda x: x['station_details'])\n",
    "    servicekontrakt_df['original_station_names'] = servicekontrakt_df['station_details'].apply(\n",
    "        lambda x: [detail['original_name'] for detail in x]\n",
    "    )\n",
    "    servicekontrakt_df['station_codes'] = conversion_results.apply(lambda x: x['station_codes'])\n",
    "    servicekontrakt_df['short_path'] = conversion_results.apply(lambda x: x['short_path'])\n",
    "    \n",
    "    return servicekontrakt_df\n",
    "\n",
    "\n",
    "def calculate_sum_langd_for_bandelnamn(row, dictionary_df):\n",
    "    # If short_path is None, return None\n",
    "    if not row['short_path']:\n",
    "        return None\n",
    "\n",
    "    # Case 1: Single station\n",
    "    if '-' not in row['short_path']:\n",
    "        single_station = row['short_path']\n",
    "        \n",
    "        # Find the row in dictionary_df where Plats_sign matches the single station\n",
    "        matching_station = dictionary_df[dictionary_df['Plats_sign'] == single_station]\n",
    "        \n",
    "        # If no matching station is found, return None\n",
    "        if matching_station.empty:\n",
    "            return None\n",
    "        \n",
    "        # Get the station's length\n",
    "        station_length = matching_station['Banlangd'].iloc[0]\n",
    "        \n",
    "        # Find neighboring connections in the Forbind column (skip NaN values)\n",
    "        neighbors = dictionary_df[\n",
    "            dictionary_df['Forbind'].notna() &  # Skip NaN values\n",
    "            dictionary_df['Forbind'].str.contains(f\"^{single_station}-|-{single_station}$\", regex=True)\n",
    "        ]\n",
    "        \n",
    "        # Calculate half the lengths of the neighboring connections\n",
    "        half_neighbor_lengths = 0\n",
    "        for _, neighbor in neighbors.iterrows():\n",
    "            length = neighbor['Banlangd']\n",
    "            half_neighbor_lengths += length / 2\n",
    "        \n",
    "        # Return the total length: station's length + half the neighbors' lengths\n",
    "        return station_length + half_neighbor_lengths\n",
    "\n",
    "    # Case 2: Multiple stations (existing logic)\n",
    "    return calculate_sum_langd(row['short_path'], int(row['Bandelnr']), dictionary_df)\n",
    "\n",
    "\n",
    "# Apply the steps\n",
    "# 1. First, prepare the conversion\n",
    "servicekontrakt_df = prepare_bandelnamn_conversion(servicekontrakt_df)\n",
    "\n",
    "# 2. Then calculate sum_langd\n",
    "servicekontrakt_df['sum_langd'] = servicekontrakt_df.apply(\n",
    "    lambda row: calculate_sum_langd_for_bandelnamn(row, dictionary_df),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Add 'kontrakt_från_bandel' column by mapping 'Bandel' to the cleaned dictionary\n",
    "def map_bandel_to_contract(bandel, mapping_dict):\n",
    "    # Extract the first Bandelnr if Bandelnr contains multiple values (e.g., \"451, 452\")\n",
    "    first_bandel = int(bandel)\n",
    "    return mapping_dict.get(first_bandel, None)\n",
    "\n",
    "servicekontrakt_df['kontrakt_från_bandel'] = servicekontrakt_df['Bandelnr'].apply(\n",
    "    lambda bandel: map_bandel_to_contract(bandel, bandel_contract_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching TCR:s Förbindelser with bandelar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us reformat the förbindelser in TCR so that it is a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Process 'Från linjespår' in tcr_df to create a 'förbind_list' column\n",
    "def extract_forbind_list(fran_linjespar):\n",
    "    # Use regex to extract patterns like 'Lub-Ttu', 'Ttu-Vån', etc.\n",
    "    pattern = r'\\b([A-Za-zåäöÅÄÖ]+-[A-Za-zåäöÅÄÖ]+)\\b'\n",
    "    matches = re.findall(pattern, fran_linjespar)\n",
    "    return ', '.join(matches)\n",
    "\n",
    "# Apply the function to create the 'förbind_list' column\n",
    "tcr_df['förbind_list'] = tcr_df['Från linjespår'].apply(lambda x: extract_forbind_list(x) if pd.notna(x) else '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now based on that list förbind_list, we can identify the list of bandelar using dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify bandelar for rows of tcr_df using 'förbind_list' and dictionary_df\n",
    "def identify_bandelar(forbind_list, fran_trafikplats, dictionary_df):\n",
    "    # Split the 'förbind_list' string into individual elements\n",
    "    forbind_list = [f.strip() for f in forbind_list.split(',')]\n",
    "    \n",
    "    for forbind in forbind_list:\n",
    "        # First try to find the exact match\n",
    "        match = dictionary_df[dictionary_df['Forbind'] == forbind]\n",
    "        if not match.empty:\n",
    "            return match.iloc[0]['BdlNr']\n",
    "        else:\n",
    "            # Try to find the reversed direction 'Y-X' instead of 'X-Y'\n",
    "            reversed_forbind = '-'.join(forbind.split('-')[::-1])\n",
    "            reversed_match = dictionary_df[dictionary_df['Forbind'] == reversed_forbind]\n",
    "            if not reversed_match.empty:\n",
    "                return reversed_match.iloc[0]['BdlNr']\n",
    "            else:\n",
    "                # Try finding using one part, either X or Y, in 'Plats_sign'\n",
    "                parts = forbind.split('-')\n",
    "                for part in parts:\n",
    "                    part_match = dictionary_df[dictionary_df['Plats_sign'] == part]\n",
    "                    if not part_match.empty:\n",
    "                        return part_match.iloc[0]['BdlNr']\n",
    "                # If still not found, try using 'Från trafikplats'\n",
    "                fran_trafikplats_match = dictionary_df[dictionary_df['Plats_sign'] == fran_trafikplats]\n",
    "                if not fran_trafikplats_match.empty:\n",
    "                    return fran_trafikplats_match.iloc[0]['BdlNr']\n",
    "    return None\n",
    "\n",
    "# Apply the function to create the 'identified_bandelar' column\n",
    "tcr_df['identified_BdlNr'] = tcr_df.apply(lambda row: identify_bandelar(row['förbind_list'], row['Från trafikplats'], dictionary_df), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some rows where the bandel is not identified because the förbind_list is empty. For these rows use column Från trafikplats to find the corresponding BdlNr using column Plats_sign of dictionary_df. The following stations are concerned:\n",
    "* M -> corresponds to Malmö central which is referred to in the dictionary as Mc\n",
    "* V -> corresponds to Värnamo which is referred to in the dictionary as Väc\n",
    "* Sär -> corresponds to Sävenäs which is referred to in the dictionary as Gsv or Göteborg Sävenäs\n",
    "* Gäb -> corresponds to Gävle GBG which is referred to in the dictionary as Gä\n",
    "\n",
    "Others not found in the dictionary such as\n",
    "* Gsh ->  Skandiahamnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rows where 'identified_BdlNr' is an empty string\n",
    "mask = tcr_df['identified_BdlNr'].isna()\n",
    "trafikplats_lookup = dictionary_df.set_index('Plats_sign')['BdlNr'].to_dict()\n",
    "\n",
    "tcr_df.loc[mask, 'identified_BdlNr'] = tcr_df.loc[mask, 'Från trafikplats'].map(trafikplats_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have identified BdlNR, we can use servicekontrakt_df to add a column with Kontraktsområdesnamn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the contract map with float conversion\n",
    "contract_map = servicekontrakt_df.drop_duplicates(subset=['Bandelnr']).copy()\n",
    "contract_map['Bandelnr'] = contract_map['Bandelnr'].astype(float)\n",
    "contract_map = contract_map.set_index('Bandelnr')['Kontraktsområdesnamn'].to_dict()\n",
    "\n",
    "# Map with the contract map\n",
    "tcr_df['identified_BdlNr'] = tcr_df['identified_BdlNr'].astype(float)\n",
    "tcr_df['Kontraktsområdesnamn'] = tcr_df['identified_BdlNr'].map(contract_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional mapping using the bandel_contract_dict\n",
    "tcr_df['kontrakt_från_bandel'] = tcr_df['identified_BdlNr'].map(bandel_contract_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No that we have identified BdlNr, we want to get the total length langd (which is in meter) of the forbind_list and put it in a column (sum_langd). The idea is to use the order of forbind_list and look for the corresponding rows in dictionary_df (within same bandelnr = identified BdlNr) and accumulate the lenght in column dictionary_df(Banlangd). The forbind_list are normally linked, e.g., A-B, B-C, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### `tcr_df` Processing ###\n",
    "\n",
    "# Apply the function to create the 'sum_langd' column for tcr_df\n",
    "tcr_df['sum_langd'] = tcr_df.apply(\n",
    "    lambda row: calculate_sum_langd(\n",
    "        row['förbind_list'], \n",
    "        row['identified_BdlNr'], \n",
    "        dictionary_df\n",
    "    ), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of the TCRs, there is empty forbind_list (i.e., \"\"), but in all cases there is a station in Från trafikplats that we can use to find the langd in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a mapping from Plats_sign (full station name) to Banlangd\n",
    "langd_lookup = dictionary_df.set_index('Plats_sign')['Banlangd'].to_dict()\n",
    "\n",
    "# Step 2: Create a mask for rows where 'förbind_list' is empty\n",
    "mask = (tcr_df['förbind_list'] == \"\")\n",
    "\n",
    "# Step 3: Lookup 'Banlangd' for 'Från trafikplats' where 'förbind_list' is empty\n",
    "tcr_df.loc[mask, 'sum_langd'] = tcr_df.loc[mask, 'Från trafikplats'].map(langd_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for excel\n",
    "excel_file_path = \"Servicekontrakt_per_bandel_matched.xlsx\"\n",
    "servicekontrakt_df_to_export = servicekontrakt_df[['Kontraktsområdesnamn', 'kontrakt_från_bandel','Tidsperiod', 'Bandel', 'TPA timmar per år',\n",
    "       'TPA dagar per år', 'TPA veckor per år', 'TPA timmar natt per år',\n",
    "       'TPA timmar helg per år', 'EJ TPA timmar per år', 'EJ TPA dagar per år',\n",
    "       'EJ TPA veckor per år', 'EJ TPA timmar natt per år',\n",
    "       'EJ TPA timmar helg per år', 'Total timmar per år', 'Bandelnr',\n",
    "       'Bandelnamn', 'sum_langd']]\n",
    "servicekontrakt_df_to_export.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Keep only the specified columns in tcr_df\n",
    "tcr_df_to_export = tcr_df[['TCR-id', 'Klassificering', 'Orsak till kapacitets-begränsning',\n",
    "                 'Starttid', 'Sluttid', 'Servicefönster_nya_kategorier',\n",
    "                 'Relaterade TPÅ:er', 'tid_timmar', 'Relaterad åtgärdsnummer', 'Relaterad kontrakt',\n",
    "                 'förbind_list', 'identified_BdlNr', 'sum_langd', 'Kontraktsområdesnamn','kontrakt_från_bandel']]\n",
    "\n",
    "# Step 10: Export the DataFrame to an Excel file\n",
    "excel_file_path = \"TCR_T24_matched_bandelar.xlsx\"\n",
    "tcr_df_to_export.to_excel(excel_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
